"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[2868],{91833:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>r,metadata:()=>l,toc:()=>d});var i=t(85893),s=t(11151);const r={sidebar_label:"retrieve_user_proxy_agent",title:"agentchat.contrib.retrieve_user_proxy_agent"},o=void 0,l={id:"reference/agentchat/contrib/retrieve_user_proxy_agent",title:"agentchat.contrib.retrieve_user_proxy_agent",description:"RetrieveUserProxyAgent",source:"@site/docs/reference/agentchat/contrib/retrieve_user_proxy_agent.md",sourceDirName:"reference/agentchat/contrib",slug:"/reference/agentchat/contrib/retrieve_user_proxy_agent",permalink:"/autogen/docs/reference/agentchat/contrib/retrieve_user_proxy_agent",draft:!1,unlisted:!1,editUrl:"https://github.com/microsoft/autogen/edit/main/website/docs/reference/agentchat/contrib/retrieve_user_proxy_agent.md",tags:[],version:"current",frontMatter:{sidebar_label:"retrieve_user_proxy_agent",title:"agentchat.contrib.retrieve_user_proxy_agent"},sidebar:"referenceSideBar",previous:{title:"retrieve_assistant_agent",permalink:"/autogen/docs/reference/agentchat/contrib/retrieve_assistant_agent"},next:{title:"society_of_mind_agent",permalink:"/autogen/docs/reference/agentchat/contrib/society_of_mind_agent"}},c={},d=[{value:"RetrieveUserProxyAgent",id:"retrieveuserproxyagent",level:2},{value:"__init__",id:"__init__",level:3},{value:"retrieve_docs",id:"retrieve_docs",level:3},{value:"message_generator",id:"message_generator",level:3}];function a(e){const n={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.a)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.h2,{id:"retrieveuserproxyagent",children:"RetrieveUserProxyAgent"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"class RetrieveUserProxyAgent(UserProxyAgent)\n"})}),"\n",(0,i.jsx)(n.p,{children:"(In preview) The Retrieval-Augmented User Proxy retrieves document chunks based on the embedding\nsimilarity, and sends them along with the question to the Retrieval-Augmented Assistant"}),"\n",(0,i.jsx)(n.h3,{id:"__init__",children:"__init__"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def __init__(name="RetrieveChatAgent",\n             human_input_mode: Literal["ALWAYS", "NEVER",\n                                       "TERMINATE"] = "ALWAYS",\n             is_termination_msg: Optional[Callable[[Dict], bool]] = None,\n             retrieve_config: Optional[Dict] = None,\n             **kwargs)\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"name"})," ",(0,i.jsx)(n.em,{children:"str"})," - name of the agent."]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"human_input_mode"})," ",(0,i.jsx)(n.em,{children:"str"}),' - whether to ask for human inputs every time a message is received.\nPossible values are "ALWAYS", "TERMINATE", "NEVER".']}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:'When "ALWAYS", the agent prompts for human input every time a message is received.\nUnder this mode, the conversation stops when the human input is "exit",\nor when is_termination_msg is True and there is no human input.'}),"\n",(0,i.jsx)(n.li,{children:'When "TERMINATE", the agent only prompts for human input only when a termination\nmessage is received or the number of auto reply reaches\nthe max_consecutive_auto_reply.'}),"\n",(0,i.jsx)(n.li,{children:'When "NEVER", the agent will never prompt for human input. Under this mode, the\nconversation stops when the number of auto reply reaches the\nmax_consecutive_auto_reply or when is_termination_msg is True.'}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"is_termination_msg"})," ",(0,i.jsx)(n.em,{children:"function"}),' - a function that takes a message in the form of a dictionary\nand returns a boolean value indicating if this received message is a termination message.\nThe dict can contain the following keys: "content", "role", "name", "function_call".']}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"retrieve_config"})," ",(0,i.jsx)(n.em,{children:"dict or None"})," - config for the retrieve agent."]}),"\n",(0,i.jsx)(n.p,{children:"To use default config, set to None. Otherwise, set to a dictionary with the\nfollowing keys:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"task"}),' (Optional, str) - the task of the retrieve chat. Possible values are\n"code", "qa" and "default". System prompt will be different for different tasks.\nThe default value is ',(0,i.jsx)(n.code,{children:"default"}),", which supports both code and qa, and provides\nsource information in the end of the response."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"vector_db"}),' (Optional, Union[str, VectorDB]) - the vector db for the retrieve chat.\nIf it\'s a string, it should be the type of the vector db, such as "chroma"; otherwise,\nit should be an instance of the VectorDB protocol. Default is "chroma".\nSet ',(0,i.jsx)(n.code,{children:"None"})," to use the deprecated ",(0,i.jsx)(n.code,{children:"client"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"db_config"})," (Optional, Dict) - the config for the vector db. Default is ",(0,i.jsx)(n.code,{children:"{}"}),". Please make\nsure you understand the config for the vector db you are using, otherwise, leave it as ",(0,i.jsx)(n.code,{children:"{}"}),".\nOnly valid when ",(0,i.jsx)(n.code,{children:"vector_db"})," is a string."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"client"})," (Optional, chromadb.Client) - the chromadb client. If key not provided, a\ndefault client ",(0,i.jsx)(n.code,{children:"chromadb.Client()"})," will be used. If you want to use other\nvector db, extend this class and override the ",(0,i.jsx)(n.code,{children:"retrieve_docs"})," function.\n",(0,i.jsx)(n.em,{children:"[Deprecated]"})," use ",(0,i.jsx)(n.code,{children:"vector_db"})," instead."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"docs_path"})," (Optional, Union[str, List[str]]) - the path to the docs directory. It\ncan also be the path to a single file, the url to a single file or a list\nof directories, files and urls. Default is None, which works only if the\ncollection is already created."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"extra_docs"}),' (Optional, bool) - when true, allows adding documents with unique IDs\nwithout overwriting existing ones; when false, it replaces existing documents\nusing default IDs, risking collection overwrite., when set to true it enables\nthe system to assign unique IDs starting from "length+i" for new document\nchunks, preventing the replacement of existing documents and facilitating the\naddition of more content to the collection..\nBy default, "extra_docs" is set to false, starting document IDs from zero.\nThis poses a risk as new documents might overwrite existing ones, potentially\ncausing unintended loss or alteration of data in the collection.\n',(0,i.jsx)(n.em,{children:"[Deprecated]"})," use ",(0,i.jsx)(n.code,{children:"new_docs"})," when use ",(0,i.jsx)(n.code,{children:"vector_db"})," instead of ",(0,i.jsx)(n.code,{children:"client"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"new_docs"})," (Optional, bool) - when True, only adds new documents to the collection;\nwhen False, updates existing documents and adds new ones. Default is True.\nDocument id is used to determine if a document is new or existing. By default, the\nid is the hash value of the content."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"model"})," (Optional, str) - the model to use for the retrieve chat.\nIf key not provided, a default model ",(0,i.jsx)(n.code,{children:"gpt-4"})," will be used."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"chunk_token_size"})," (Optional, int) - the chunk token size for the retrieve chat.\nIf key not provided, a default size ",(0,i.jsx)(n.code,{children:"max_tokens * 0.4"})," will be used."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"context_max_tokens"})," (Optional, int) - the context max token size for the\nretrieve chat.\nIf key not provided, a default size ",(0,i.jsx)(n.code,{children:"max_tokens * 0.8"})," will be used."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"chunk_mode"}),' (Optional, str) - the chunk mode for the retrieve chat. Possible values\nare "multi_lines" and "one_line". If key not provided, a default mode\n',(0,i.jsx)(n.code,{children:"multi_lines"})," will be used."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"must_break_at_empty_line"}),' (Optional, bool) - chunk will only break at empty line\nif True. Default is True.\nIf chunk_mode is "one_line", this parameter will be ignored.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"embedding_model"})," (Optional, str) - the embedding model to use for the retrieve chat.\nIf key not provided, a default model ",(0,i.jsx)(n.code,{children:"all-MiniLM-L6-v2"})," will be used. All available\nmodels can be found at ",(0,i.jsx)(n.code,{children:"https://www.sbert.net/docs/pretrained_models.html"}),".\nThe default model is a fast model. If you want to use a high performance model,\n",(0,i.jsx)(n.code,{children:"all-mpnet-base-v2"})," is recommended.\n",(0,i.jsx)(n.em,{children:"[Deprecated]"})," no need when use ",(0,i.jsx)(n.code,{children:"vector_db"})," instead of ",(0,i.jsx)(n.code,{children:"client"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"embedding_function"})," (Optional, Callable) - the embedding function for creating the\nvector db. Default is None, SentenceTransformer with the given ",(0,i.jsx)(n.code,{children:"embedding_model"}),"\nwill be used. If you want to use OpenAI, Cohere, HuggingFace or other embedding\nfunctions, you can pass it here,\nfollow the examples in ",(0,i.jsx)(n.code,{children:"https://docs.trychroma.com/embeddings"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"customized_prompt"})," (Optional, str) - the customized prompt for the retrieve chat.\nDefault is None."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"customized_answer_prefix"}),' (Optional, str) - the customized answer prefix for the\nretrieve chat. Default is "".\nIf not "" and the customized_answer_prefix is not in the answer,\n',(0,i.jsx)(n.code,{children:"Update Context"})," will be triggered."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"update_context"})," (Optional, bool) - if False, will not apply ",(0,i.jsx)(n.code,{children:"Update Context"})," for\ninteractive retrieval. Default is True."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"collection_name"})," (Optional, str) - the name of the collection.\nIf key not provided, a default name ",(0,i.jsx)(n.code,{children:"autogen-docs"})," will be used."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"get_or_create"})," (Optional, bool) - Whether to get the collection if it exists. Default is False."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"overwrite"})," (Optional, bool) - Whether to overwrite the collection if it exists. Default is False.\nCase 1. if the collection does not exist, create the collection.\nCase 2. the collection exists, if overwrite is True, it will overwrite the collection.\nCase 3. the collection exists and overwrite is False, if get_or_create is True, it will get the collection,\notherwise it raise a ValueError."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"custom_token_count_function"}),' (Optional, Callable) - a custom function to count the\nnumber of tokens in a string.\nThe function should take (text:str, model:str) as input and return the\ntoken_count(int). the retrieve_config["model"] will be passed in the function.\nDefault is autogen.token_count_utils.count_token that uses tiktoken, which may\nnot be accurate for non-OpenAI models.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"custom_text_split_function"})," (Optional, Callable) - a custom function to split a\nstring into a list of strings.\nDefault is None, will use the default function in\n",(0,i.jsx)(n.code,{children:"autogen.retrieve_utils.split_text_to_chunks"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"custom_text_types"})," (Optional, List[str]) - a list of file types to be processed.\nDefault is ",(0,i.jsx)(n.code,{children:"autogen.retrieve_utils.TEXT_FORMATS"}),".\nThis only applies to files under the directories in ",(0,i.jsx)(n.code,{children:"docs_path"}),". Explicitly\nincluded files and urls will be chunked regardless of their types."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"recursive"})," (Optional, bool) - whether to search documents recursively in the\ndocs_path. Default is True."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"distance_threshold"})," (Optional, float) - the threshold for the distance score, only\ndistance smaller than it will be returned. Will be ignored if < 0. Default is -1."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.code,{children:"**kwargs"})," ",(0,i.jsx)(n.em,{children:"dict"})," - other kwargs in ",(0,i.jsx)(n.a,{href:"../user_proxy_agent#__init__",children:"UserProxyAgent"}),"."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Example"}),":"]}),"\n",(0,i.jsxs)(n.p,{children:["Example of overriding retrieve_docs - If you have set up a customized vector db, and it's\nnot compatible with chromadb, you can easily plug in it with below code.\n",(0,i.jsx)(n.em,{children:"[Deprecated]"})," use ",(0,i.jsx)(n.code,{children:"vector_db"})," instead. You can extend VectorDB and pass it to the agent."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'class MyRetrieveUserProxyAgent(RetrieveUserProxyAgent):\n    def query_vector_db(\n        self,\n        query_texts: List[str],\n        n_results: int = 10,\n        search_string: str = "",\n        **kwargs,\n    ) -> Dict[str, Union[List[str], List[List[str]]]]:\n        # define your own query function here\n        pass\n\n    def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = "", **kwargs):\n        results = self.query_vector_db(\n            query_texts=[problem],\n            n_results=n_results,\n            search_string=search_string,\n            **kwargs,\n        )\n\n        self._results = results\n        print("doc_ids: ", results["ids"])\n'})}),"\n",(0,i.jsx)(n.h3,{id:"retrieve_docs",children:"retrieve_docs"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'def retrieve_docs(problem: str, n_results: int = 20, search_string: str = "")\n'})}),"\n",(0,i.jsxs)(n.p,{children:["Retrieve docs based on the given problem and assign the results to the class property ",(0,i.jsx)(n.code,{children:"_results"}),".\nThe retrieved docs should be type of ",(0,i.jsx)(n.code,{children:"QueryResults"})," which is a list of tuples containing the document and\nthe distance."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"problem"})," ",(0,i.jsx)(n.em,{children:"str"})," - the problem to be solved."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"n_results"})," ",(0,i.jsx)(n.em,{children:"int"})," - the number of results to be retrieved. Default is 20."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"search_string"})," ",(0,i.jsx)(n.em,{children:"str"}),' - only docs that contain an exact match of this string will be retrieved. Default is "".\nNot used if the vector_db doesn\'t support it.']}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Returns"}),":"]}),"\n",(0,i.jsx)(n.p,{children:"None."}),"\n",(0,i.jsx)(n.h3,{id:"message_generator",children:"message_generator"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"@staticmethod\ndef message_generator(sender, recipient, context)\n"})}),"\n",(0,i.jsx)(n.p,{children:"Generate an initial message with the given context for the RetrieveUserProxyAgent."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"sender"})," ",(0,i.jsx)(n.em,{children:"Agent"})," - the sender agent. It should be the instance of RetrieveUserProxyAgent."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"recipient"})," ",(0,i.jsx)(n.em,{children:"Agent"})," - the recipient agent. Usually it's the assistant agent."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"context"})," ",(0,i.jsx)(n.em,{children:"dict"})," - the context for the message generation. It should contain the following keys:\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"problem"})," (str) - the problem to be solved."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"n_results"})," (int) - the number of results to be retrieved. Default is 20."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"search_string"}),' (str) - only docs that contain an exact match of this string will be retrieved. Default is "".']}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Returns"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.code,{children:"str"})," - the generated message ready to be sent to the recipient agent."]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(a,{...e})}):a(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>l,a:()=>o});var i=t(67294);const s={},r=i.createContext(s);function o(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);