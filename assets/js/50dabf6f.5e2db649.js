"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[6162],{23681:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>r,contentTitle:()=>o,default:()=>d,frontMatter:()=>a,metadata:()=>c,toc:()=>l});var t=i(85893),s=i(11151);const a={sidebar_label:"vision_capability",title:"agentchat.contrib.capabilities.vision_capability"},o=void 0,c={id:"reference/agentchat/contrib/capabilities/vision_capability",title:"agentchat.contrib.capabilities.vision_capability",description:"VisionCapability",source:"@site/docs/reference/agentchat/contrib/capabilities/vision_capability.md",sourceDirName:"reference/agentchat/contrib/capabilities",slug:"/reference/agentchat/contrib/capabilities/vision_capability",permalink:"/autogen/docs/reference/agentchat/contrib/capabilities/vision_capability",draft:!1,unlisted:!1,editUrl:"https://github.com/microsoft/autogen/edit/main/website/docs/reference/agentchat/contrib/capabilities/vision_capability.md",tags:[],version:"current",frontMatter:{sidebar_label:"vision_capability",title:"agentchat.contrib.capabilities.vision_capability"},sidebar:"referenceSideBar",previous:{title:"transforms_util",permalink:"/autogen/docs/reference/agentchat/contrib/capabilities/transforms_util"},next:{title:"base",permalink:"/autogen/docs/reference/agentchat/contrib/vectordb/base"}},r={},l=[{value:"VisionCapability",id:"visioncapability",level:2},{value:"__init__",id:"__init__",level:3},{value:"process_last_received_message",id:"process_last_received_message",level:3}];function h(e){const n={a:"a",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h2,{id:"visioncapability",children:"VisionCapability"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"class VisionCapability(AgentCapability)\n"})}),"\n",(0,t.jsx)(n.p,{children:"We can add vision capability to regular ConversableAgent, even if the agent does not have the multimodal capability,\nsuch as GPT-3.5-turbo agent, Llama, Orca, or Mistral agents. This vision capability will invoke a LMM client to describe\nthe image (captioning) before sending the information to the agent's actual client."}),"\n",(0,t.jsxs)(n.p,{children:["The vision capability will hook to the ConversableAgent's ",(0,t.jsx)(n.code,{children:"process_last_received_message"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"Some technical details:\nWhen the agent (who has the vision capability) received an message, it will:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"_process_received_message:\na. _append_oai_message"}),"\n",(0,t.jsx)(n.li,{children:"generate_reply: if the agent is a MultimodalAgent, it will also use the image tag.\na. hook process_last_received_message (NOTE: this is where the vision capability will be hooked to.)\nb. hook process_all_messages_before_reply"}),"\n",(0,t.jsx)(n.li,{children:"send:\na. hook process_message_before_send\nb. _append_oai_message"}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"__init__",children:"__init__"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def __init__(lmm_config: Dict,\n             description_prompt: Optional[str] = DEFAULT_DESCRIPTION_PROMPT,\n             custom_caption_func: Callable = None) -> None\n"})}),"\n",(0,t.jsx)(n.p,{children:"Initializes a new instance, setting up the configuration for interacting with\na Language Multimodal (LMM) client and specifying optional parameters for image\ndescription and captioning."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"lmm_config"})," ",(0,t.jsx)(n.em,{children:"Dict"})," - Configuration for the LMM client, which is used to call\nthe LMM service for describing the image. This must be a dictionary containing\nthe necessary configuration parameters. If ",(0,t.jsx)(n.code,{children:"lmm_config"})," is False or an empty dictionary,\nit is considered invalid, and initialization will assert."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"description_prompt"})," ",(0,t.jsx)(n.em,{children:"Optional[str], optional"})," - The prompt to use for generating\ndescriptions of the image. This parameter allows customization of the\nprompt passed to the LMM service. Defaults to ",(0,t.jsx)(n.code,{children:"DEFAULT_DESCRIPTION_PROMPT"})," if not provided."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"custom_caption_func"})," ",(0,t.jsx)(n.em,{children:"Callable, optional"})," - A callable that, if provided, will be used\nto generate captions for images. This allows for custom captioning logic outside\nof the standard LMM service interaction.\nThe callable should take three parameters as input:\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"an image URL (or local location)"}),"\n",(0,t.jsx)(n.li,{children:"image_data (a PIL image)"}),"\n",(0,t.jsxs)(n.li,{children:["lmm_client (to call remote LMM)\nand then return a description (as string).\nIf not provided, captioning will rely on the LMM client configured via ",(0,t.jsx)(n.code,{children:"lmm_config"}),".\nIf provided, we will not run the default self._get_image_caption method."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Raises"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"AssertionError"})," - If neither a valid ",(0,t.jsx)(n.code,{children:"lmm_config"})," nor a ",(0,t.jsx)(n.code,{children:"custom_caption_func"})," is provided,\nan AssertionError is raised to indicate that the Vision Capability requires\none of these to be valid for operation."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"process_last_received_message",children:"process_last_received_message"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"def process_last_received_message(content: Union[str, List[dict]]) -> str\n"})}),"\n",(0,t.jsx)(n.p,{children:"Processes the last received message content by normalizing and augmenting it\nwith descriptions of any included images. The function supports input content\nas either a string or a list of dictionaries, where each dictionary represents\na content item (e.g., text, image). If the content contains image URLs, it\nfetches the image data, generates a caption for each image, and inserts the\ncaption into the augmented content."}),"\n",(0,t.jsx)(n.p,{children:"The function aims to transform the content into a format compatible with GPT-4V\nmultimodal inputs, specifically by formatting strings into PIL-compatible\nimages if needed and appending text descriptions for images. This allows for\na more accessible presentation of the content, especially in contexts where\nimages cannot be displayed directly."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Arguments"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"content"})," ",(0,t.jsx)(n.em,{children:"Union[str, List[dict]]"})," - The last received message content, which\ncan be a plain text string or a list of dictionaries representing\ndifferent types of content items (e.g., text, image_url)."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Returns"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"str"})," - The augmented message content"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Raises"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"AssertionError"})," - If an item in the content list is not a dictionary."]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Examples"}),":"]}),"\n",(0,t.jsxs)(n.p,{children:["Assuming ",(0,t.jsx)(n.code,{children:"self._get_image_caption(img_data)"}),' returns\n"A beautiful sunset over the mountains" for the image.']}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsx)(n.p,{children:'Input as String:\ncontent = "Check out this cool photo!"'}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"Output"}),' - "Check out this cool photo!"\n(Content is a string without an image, remains unchanged.)']}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["Input as String, with image location:\ncontent = \"What's weather in this cool photo: <img ",(0,t.jsx)(n.a,{href:"http://example.com/photo.jpg%3E",children:"http://example.com/photo.jpg>"}),'"']}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"Output"})," - \"What's weather in this cool photo: <img ",(0,t.jsx)(n.a,{href:"http://example.com/photo.jpg%3E",children:"http://example.com/photo.jpg>"}),' in case you can not see, the caption of this image is:\nA beautiful sunset over the mountains\n"\n(Caption added after the image)']}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'Input as List with Text Only:\ncontent = [{"type": "text", "text": "Here\'s an interesting fact."}]'}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"Output"}),' - "Here\'s an interesting fact."\n(No images in the content, it remains unchanged.)']}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Input as List with Image URL:\ncontent = ["}),"\n"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:'{"type"'}),' - "text", "text": "What\'s weather in this cool photo:"},']}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:'{"type"'}),' - "image_url", "image_url": {"url": "',(0,t.jsx)(n.a,{href:"http://example.com/photo.jpg%22%7D%7D",children:'http://example.com/photo.jpg"}}'}),"\n]"]}),"\n"]}),"\n",(0,t.jsxs)(n.li,{children:["\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.code,{children:"Output"})," - \"What's weather in this cool photo: <img ",(0,t.jsx)(n.a,{href:"http://example.com/photo.jpg%3E",children:"http://example.com/photo.jpg>"}),' in case you can not see, the caption of this image is:\nA beautiful sunset over the mountains\n"\n(Caption added after the image)']}),"\n"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(h,{...e})}):h(e)}},11151:(e,n,i)=>{i.d(n,{Z:()=>c,a:()=>o});var t=i(67294);const s={},a=t.createContext(s);function o(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);