"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[9736],{11643:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>m,frontMatter:()=>i,metadata:()=>r,toc:()=>l});var s=t(85893),o=t(11151);const i={},a="Compressing Text with LLMLingua",r={id:"topics/handling_long_contexts/compressing_text_w_llmligua",title:"Compressing Text with LLMLingua",description:"Text compression is crucial for optimizing interactions with LLMs, especially when dealing with long prompts that can lead to higher costs and slower response times. LLMLingua is a tool designed to compress prompts effectively, enhancing the efficiency and cost-effectiveness of LLM operations.",source:"@site/docs/topics/handling_long_contexts/compressing_text_w_llmligua.md",sourceDirName:"topics/handling_long_contexts",slug:"/topics/handling_long_contexts/compressing_text_w_llmligua",permalink:"/autogen/docs/topics/handling_long_contexts/compressing_text_w_llmligua",draft:!1,unlisted:!1,editUrl:"https://github.com/microsoft/autogen/edit/main/website/docs/topics/handling_long_contexts/compressing_text_w_llmligua.md",tags:[],version:"current",frontMatter:{},sidebar:"docsSidebar",previous:{title:"vLLM",permalink:"/autogen/docs/topics/non-openai-models/local-vllm"},next:{title:"Introduction to Transform Messages",permalink:"/autogen/docs/topics/handling_long_contexts/intro_to_transform_messages"}},c={},l=[{value:"Example 1: Compressing AutoGen Research Paper using LLMLingua",id:"example-1-compressing-autogen-research-paper-using-llmlingua",level:2},{value:"Example 2: Integrating LLMLingua with <code>ConversableAgent</code>",id:"example-2-integrating-llmlingua-with-conversableagent",level:2},{value:"Example 3: Modifying LLMLingua&#39;s Compression Parameters",id:"example-3-modifying-llmlinguas-compression-parameters",level:2}];function p(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,o.a)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.h1,{id:"compressing-text-with-llmlingua",children:"Compressing Text with LLMLingua"}),"\n",(0,s.jsx)(n.p,{children:"Text compression is crucial for optimizing interactions with LLMs, especially when dealing with long prompts that can lead to higher costs and slower response times. LLMLingua is a tool designed to compress prompts effectively, enhancing the efficiency and cost-effectiveness of LLM operations."}),"\n",(0,s.jsx)(n.p,{children:"This guide introduces LLMLingua's integration with AutoGen, demonstrating how to use this tool to compress text, thereby optimizing the usage of LLMs for various applications."}),"\n",(0,s.jsxs)(n.admonition,{title:"Requirements",type:"info",children:[(0,s.jsxs)(n.p,{children:["Install ",(0,s.jsx)(n.code,{children:"pyautogen[long-context]"})," and ",(0,s.jsx)(n.code,{children:"PyMuPDF"}),":"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'pip install "pyautogen[long-context]" PyMuPDF\n'})}),(0,s.jsxs)(n.p,{children:["For more information, please refer to the ",(0,s.jsx)(n.a,{href:"/docs/installation/",children:"installation guide"}),"."]})]}),"\n",(0,s.jsx)(n.h2,{id:"example-1-compressing-autogen-research-paper-using-llmlingua",children:"Example 1: Compressing AutoGen Research Paper using LLMLingua"}),"\n",(0,s.jsxs)(n.p,{children:["We will look at how we can use ",(0,s.jsx)(n.code,{children:"TextMessageCompressor"})," to compress an AutoGen research paper using ",(0,s.jsx)(n.code,{children:"LLMLingua"}),". Here's how you can initialize ",(0,s.jsx)(n.code,{children:"TextMessageCompressor"})," with LLMLingua, a text compressor that adheres to the ",(0,s.jsx)(n.code,{children:"TextCompressor"})," protocol."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import tempfile\n\nimport fitz  # PyMuPDF\nimport requests\n\nfrom autogen.agentchat.contrib.capabilities.text_compressors import LLMLingua\nfrom autogen.agentchat.contrib.capabilities.transforms import TextMessageCompressor\n\nAUTOGEN_PAPER = "https://arxiv.org/pdf/2308.08155"\n\n\ndef extract_text_from_pdf():\n    # Download the PDF\n    response = requests.get(AUTOGEN_PAPER)\n    response.raise_for_status()  # Ensure the download was successful\n\n    text = ""\n    # Save the PDF to a temporary file\n    with tempfile.TemporaryDirectory() as temp_dir:\n        with open(temp_dir + "temp.pdf", "wb") as f:\n            f.write(response.content)\n\n        # Open the PDF\n        with fitz.open(temp_dir + "temp.pdf") as doc:\n            # Read and extract text from each page\n            for page in doc:\n                text += page.get_text()\n\n    return text\n\n\n# Example usage\npdf_text = extract_text_from_pdf()\n\nllm_lingua = LLMLingua()\ntext_compressor = TextMessageCompressor(text_compressor=llm_lingua)\ncompressed_text = text_compressor.apply_transform([{"content": pdf_text}])\n\nprint(text_compressor.get_logs([], []))\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-console",children:"('19765 tokens saved with text compression.', True)\n"})}),"\n",(0,s.jsxs)(n.h2,{id:"example-2-integrating-llmlingua-with-conversableagent",children:["Example 2: Integrating LLMLingua with ",(0,s.jsx)(n.code,{children:"ConversableAgent"})]}),"\n",(0,s.jsxs)(n.p,{children:["Now, let's integrate ",(0,s.jsx)(n.code,{children:"LLMLingua"})," into a conversational agent within AutoGen. This allows dynamic compression of prompts before they are sent to the LLM."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'import os\n\nimport autogen\nfrom autogen.agentchat.contrib.capabilities import transform_messages\n\nsystem_message = "You are a world class researcher."\nconfig_list = [{"model": "gpt-4-turbo", "api_key": os.getenv("OPENAI_API_KEY")}]\n\n# Define your agent; the user proxy and an assistant\nresearcher = autogen.ConversableAgent(\n    "assistant",\n    llm_config={"config_list": config_list},\n    max_consecutive_auto_reply=1,\n    system_message=system_message,\n    human_input_mode="NEVER",\n)\nuser_proxy = autogen.UserProxyAgent(\n    "user_proxy",\n    human_input_mode="NEVER",\n    is_termination_msg=lambda x: "TERMINATE" in x.get("content", ""),\n    max_consecutive_auto_reply=1,\n)\n'})}),"\n",(0,s.jsx)(n.admonition,{type:"tip",children:(0,s.jsxs)(n.p,{children:["Learn more about configuring LLMs for agents ",(0,s.jsx)(n.a,{href:"/docs/topics/llm_configuration",children:"here"}),"."]})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'context_handling = transform_messages.TransformMessages(transforms=[text_compressor])\ncontext_handling.add_to_agent(researcher)\n\nmessage = "Summarize this research paper for me, include the important information" + pdf_text\nresult = user_proxy.initiate_chat(recipient=researcher, clear_history=True, message=message, silent=True)\n\nprint(result.chat_history[1]["content"])\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-console",children:"19953 tokens saved with text compression.\nThe paper describes AutoGen, a framework designed to facilitate the development of diverse large language model (LLM) applications through conversational multi-agent systems. The framework emphasizes customization and flexibility, enabling developers to define agent interaction behaviors in natural language or computer code.\n\nKey components of AutoGen include:\n1. **Conversable Agents**: These are customizable agents designed to operate autonomously or through human interaction. They are capable of initiating, maintaining, and responding within conversations, contributing effectively to multi-agent dialogues.\n\n2. **Conversation Programming**: AutoGen introduces a programming paradigm centered around conversational interactions among agents. This approach simplifies the development of complex applications by streamlining how agents communicate and interact, focusing on conversational logic rather than traditional coding for\nmats.\n\n3. **Agent Customization and Flexibility**: Developers have the freedom to define the capabilities and behaviors of agents within the system, allowing for a wide range of applications across different domains.\n\n4. **Application Versatility**: The paper outlines various use cases from mathematics and coding to decision-making and entertainment, demonstrating AutoGen's ability to cope with a broad spectrum of complexities and requirements.\n\n5. **Hierarchical and Joint Chat Capabilities**: The system supports complex conversation patterns including hierarchical and multi-agent interactions, facilitating robust dialogues that can dynamically adjust based on the conversation context and the agents' roles.\n\n6. **Open-source and Community Engagement**: AutoGen is presented as an open-source framework, inviting contributions and adaptations from the global development community to expand its capabilities and applications.\n\nThe framework's architecture is designed so that it can be seamlessly integrated into existing systems, providing a robust foundation for developing sophisticated multi-agent applications that leverage the capabilities of modern LLMs. The paper also discusses potential ethical considerations and future improvements, highlighting the importance of continual development in response to evolving tech landscapes and user needs.\n"})}),"\n",(0,s.jsx)(n.h2,{id:"example-3-modifying-llmlinguas-compression-parameters",children:"Example 3: Modifying LLMLingua's Compression Parameters"}),"\n",(0,s.jsx)(n.p,{children:"LLMLingua's flexibility allows for various configurations, such as customizing instructions for the LLM or setting specific token counts for compression. This example demonstrates how to set a target token count, enabling the use of models with smaller context sizes like gpt-3.5."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'config_list = [{"model": "gpt-3.5-turbo", "api_key": os.getenv("OPENAI_API_KEY")}]\nresearcher = autogen.ConversableAgent(\n    "assistant",\n    llm_config={"config_list": config_list},\n    max_consecutive_auto_reply=1,\n    system_message=system_message,\n    human_input_mode="NEVER",\n)\n\ntext_compressor = TextMessageCompressor(\n    text_compressor=llm_lingua,\n    compression_params={"target_token": 13000},\n    cache=None,\n)\ncontext_handling = transform_messages.TransformMessages(transforms=[text_compressor])\ncontext_handling.add_to_agent(researcher)\n\ncompressed_text = text_compressor.apply_transform([{"content": message}])\n\nresult = user_proxy.initiate_chat(recipient=researcher, clear_history=True, message=message, silent=True)\n\nprint(result.chat_history[1]["content"])\n'})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-console",children:"25308 tokens saved with text compression.\nBased on the extensive research paper information provided, it seems that the focus is on developing a framework called AutoGen for creating multi-agent conversations based on Large Language Models (LLMs) for a variety of applications such as math problem solving, coding, decision-making, and more.\n\nThe paper discusses the importance of incorporating diverse roles of LLMs, human inputs, and tools to enhance the capabilities of the conversable agents within the AutoGen framework. It also delves into the effectiveness of different systems in various scenarios, showcases the implementation of AutoGen in pilot studies, and compares its performance with other systems in tasks like math problem-solving, coding, and decision-making.\n\nThe paper also highlights the different features and components of AutoGen such as the AssistantAgent, UserProxyAgent, ExecutorAgent, and GroupChatManager, emphasizing its flexibility, ease of use, and modularity in managing multi-agent interactions. It presents case analyses to demonstrate the effectiveness of AutoGen in various applications and scenarios.\n\nFurthermore, the paper includes manual evaluations, scenario testing, code examples, and detailed comparisons with other systems like ChatGPT, OptiGuide, MetaGPT, and more, to showcase the performance and capabilities of the AutoGen framework.\n\nOverall, the research paper showcases the potential of AutoGen in facilitating dynamic multi-agent conversations, enhancing decision-making processes, and improving problem-solving tasks with the integration of LLMs, human inputs, and tools in a collaborative framework.\n"})})]})}function m(e={}){const{wrapper:n}={...(0,o.a)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},11151:(e,n,t)=>{t.d(n,{Z:()=>r,a:()=>a});var s=t(67294);const o={},i=s.createContext(o);function a(e){const n=s.useContext(i);return s.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:a(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);