"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[9385],{7658:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>i,default:()=>p,frontMatter:()=>a,metadata:()=>r,toc:()=>l});var t=o(85893),s=o(11151);const a={},i="Tips for Non-OpenAI Models",r={id:"topics/non-openai-models/best-tips-for-nonopenai-models",title:"Tips for Non-OpenAI Models",description:"Here are some tips for using non-OpenAI Models with AutoGen.",source:"@site/docs/topics/non-openai-models/best-tips-for-nonopenai-models.md",sourceDirName:"topics/non-openai-models",slug:"/topics/non-openai-models/best-tips-for-nonopenai-models",permalink:"/autogen/docs/topics/non-openai-models/best-tips-for-nonopenai-models",draft:!1,unlisted:!1,editUrl:"https://github.com/microsoft/autogen/edit/main/website/docs/topics/non-openai-models/best-tips-for-nonopenai-models.md",tags:[],version:"current",frontMatter:{},sidebar:"docsSidebar",previous:{title:"Non-OpenAI Models",permalink:"/autogen/docs/topics/non-openai-models/about-using-nonopenai-models"},next:{title:"Anthropic Claude",permalink:"/autogen/docs/topics/non-openai-models/cloud-anthropic"}},c={},l=[{value:"Finding the right model",id:"finding-the-right-model",level:2},{value:"Validating your program",id:"validating-your-program",level:2},{value:"Chat template",id:"chat-template",level:2},{value:"Discord",id:"discord",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",p:"p",pre:"pre",...(0,s.a)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.h1,{id:"tips-for-non-openai-models",children:"Tips for Non-OpenAI Models"}),"\n",(0,t.jsx)(n.p,{children:"Here are some tips for using non-OpenAI Models with AutoGen."}),"\n",(0,t.jsx)(n.h2,{id:"finding-the-right-model",children:"Finding the right model"}),"\n",(0,t.jsx)(n.p,{children:"Every model will perform differently across the operations within your AutoGen\nsetup, such as speaker selection, coding, function calling, content creation,\netc. On the whole, larger models (13B+) perform better with following directions\nand providing more cohesive responses."}),"\n",(0,t.jsx)(n.p,{children:"Content creation can be performed by most models."}),"\n",(0,t.jsx)(n.p,{children:"Fine-tuned models can be great for very specific tasks, such as function calling\nand coding."}),"\n",(0,t.jsx)(n.p,{children:"Specific tasks, such as speaker selection in a Group Chat scenario, that require\nvery accurate outputs can be a challenge with most open source/weight models. The\nuse of chain-of-thought and/or few-shot prompting can help guide the LLM to provide\nthe output in the format you want."}),"\n",(0,t.jsx)(n.h2,{id:"validating-your-program",children:"Validating your program"}),"\n",(0,t.jsx)(n.p,{children:"Testing your AutoGen setup against a very large LLM, such as OpenAI's ChatGPT or\nAnthropic's Claude 3, can help validate your agent setup and configuration."}),"\n",(0,t.jsx)(n.p,{children:"Once a setup is performing as you want, you can replace the models for your agents\nwith non-OpenAI models and iteratively tweak system messages, prompts, and model\nselection."}),"\n",(0,t.jsx)(n.h2,{id:"chat-template",children:"Chat template"}),"\n",(0,t.jsxs)(n.p,{children:["AutoGen utilises a set of chat messages for the conversation between AutoGen/user\nand LLMs. Each chat message has a role attribute that is typically ",(0,t.jsx)(n.code,{children:"user"}),",\n",(0,t.jsx)(n.code,{children:"assistant"}),", or ",(0,t.jsx)(n.code,{children:"system"}),"."]}),"\n",(0,t.jsx)(n.p,{children:"A chat template is applied during inference and some chat templates implement rules about\nwhat roles can be used in specific sequences of messages."}),"\n",(0,t.jsxs)(n.p,{children:["For example, when using Mistral AI's API the last chat message must have a role of ",(0,t.jsx)(n.code,{children:"user"}),".\nIn a Group Chat scenario the message used to select the next speaker will have a role of\n",(0,t.jsx)(n.code,{children:"system"})," by default and the API will throw an exception for this step. To overcome this the\nGroupChat's constructor has a parameter called ",(0,t.jsx)(n.code,{children:"role_for_select_speaker_messages"})," that can\nbe used to change the role name to ",(0,t.jsx)(n.code,{children:"user"}),"."]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"groupchat = autogen.GroupChat(\n    agents=[user_proxy, coder, pm],\n    messages=[],\n    max_round=12,\n    # Role for select speaker message will be set to 'user' instead of 'system'\n    role_for_select_speaker_messages='user',\n)\n"})}),"\n",(0,t.jsxs)(n.p,{children:["If the chat template associated with a model you want to use doesn't support the role\nsequence and names used in AutoGen you can modify the chat template. See an example of\nthis on our ",(0,t.jsx)(n.a,{href:"/docs/topics/non-openai-models/local-vllm#chat-template",children:"vLLM page"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"discord",children:"Discord"}),"\n",(0,t.jsxs)(n.p,{children:["Join AutoGen's ",(0,t.jsx)(n.a,{href:"https://discord.com/channels/1153072414184452236/1201369716057440287",children:"#alt-models"}),"\nchannel on their Discord and discuss non-OpenAI models and configurations."]})]})}function p(e={}){const{wrapper:n}={...(0,s.a)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},11151:(e,n,o)=>{o.d(n,{Z:()=>r,a:()=>i});var t=o(67294);const s={},a=t.createContext(s);function i(e){const n=t.useContext(a);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),t.createElement(a.Provider,{value:n},e.children)}}}]);