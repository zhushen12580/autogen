"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1623],{36831:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>c,frontMatter:()=>o,metadata:()=>r,toc:()=>d});var a=t(85893),i=t(11151);const o={custom_edit_url:"https://github.com/microsoft/autogen/edit/main/notebook/agentchat_lmm_gpt-4v.ipynb",description:"In AutoGen, leveraging multimodal models can be done through two different methodologies: MultimodalConversableAgent and VisionCapability.",source_notebook:"/notebook/agentchat_lmm_gpt-4v.ipynb",tags:["multimodal","gpt-4v"],title:"Engaging with Multimodal Models: GPT-4V in AutoGen"},s="Engaging with Multimodal Models: GPT-4V in AutoGen",r={id:"notebooks/agentchat_lmm_gpt-4v",title:"Engaging with Multimodal Models: GPT-4V in AutoGen",description:"In AutoGen, leveraging multimodal models can be done through two different methodologies: MultimodalConversableAgent and VisionCapability.",source:"@site/docs/notebooks/agentchat_lmm_gpt-4v.mdx",sourceDirName:"notebooks",slug:"/notebooks/agentchat_lmm_gpt-4v",permalink:"/autogen/docs/notebooks/agentchat_lmm_gpt-4v",draft:!1,unlisted:!1,editUrl:"https://github.com/microsoft/autogen/edit/main/notebook/agentchat_lmm_gpt-4v.ipynb",tags:[{label:"multimodal",permalink:"/autogen/docs/tags/multimodal"},{label:"gpt-4v",permalink:"/autogen/docs/tags/gpt-4-v"}],version:"current",frontMatter:{custom_edit_url:"https://github.com/microsoft/autogen/edit/main/notebook/agentchat_lmm_gpt-4v.ipynb",description:"In AutoGen, leveraging multimodal models can be done through two different methodologies: MultimodalConversableAgent and VisionCapability.",source_notebook:"/notebook/agentchat_lmm_gpt-4v.ipynb",tags:["multimodal","gpt-4v"],title:"Engaging with Multimodal Models: GPT-4V in AutoGen"},sidebar:"notebooksSidebar",previous:{title:"Generate Dalle Images With Conversable Agents",permalink:"/autogen/docs/notebooks/agentchat_image_generation_capability"},next:{title:"Runtime Logging with AutoGen",permalink:"/autogen/docs/notebooks/agentchat_logging"}},l={},d=[{value:"Before everything starts, install AutoGen with the <code>lmm</code> option",id:"before-everything-starts-install-autogen-with-the-lmm-option",level:3},{value:"Vision Capability: Group Chat Example with Multimodal Agent",id:"vision-capability-group-chat-example-with-multimodal-agent",level:2},{value:"Behavior with and without VisionCapability for Agents",id:"behavior-with-and-without-visioncapability-for-agents",level:2},{value:"Custom Caption Function for Vision Capability",id:"custom-caption-function-for-vision-capability",level:2}];function h(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.a)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.h1,{id:"engaging-with-multimodal-models-gpt-4v-in-autogen",children:"Engaging with Multimodal Models: GPT-4V in AutoGen"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.a,{href:"https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_lmm_gpt-4v.ipynb",children:(0,a.jsx)(n.img,{src:"https://colab.research.google.com/assets/colab-badge.svg",alt:"Open In Colab"})}),"\n",(0,a.jsx)(n.a,{href:"https://github.com/microsoft/autogen/blob/main/notebook/agentchat_lmm_gpt-4v.ipynb",children:(0,a.jsx)(n.img,{src:"https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github",alt:"Open on GitHub"})})]}),"\n",(0,a.jsxs)(n.p,{children:["In AutoGen, leveraging multimodal models can be done through two\ndifferent methodologies: 1. ",(0,a.jsx)(n.strong,{children:"MultimodalAgent"}),": Supported by GPT-4V and\nother LMMs, this agent is endowed with visual cognitive abilities,\nallowing it to engage in interactions comparable to those of other\nConversableAgents. 2. ",(0,a.jsx)(n.strong,{children:"VisionCapability"}),": For LLM-based agents lacking\ninherent visual comprehension, we introduce vision capabilities by\nconverting images into descriptive captions."]}),"\n",(0,a.jsx)(n.p,{children:"This guide will delve into each approach, providing insights into their\napplication and integration."}),"\n",(0,a.jsxs)(n.h3,{id:"before-everything-starts-install-autogen-with-the-lmm-option",children:["Before everything starts, install AutoGen with the ",(0,a.jsx)(n.code,{children:"lmm"})," option"]}),"\n",(0,a.jsxs)(n.p,{children:["Install ",(0,a.jsx)(n.code,{children:"pyautogen"}),":"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'pip install "pyautogen[lmm]>=0.2.17"\n'})}),"\n",(0,a.jsxs)(n.p,{children:["For more information, please refer to the ",(0,a.jsx)(n.a,{href:"../../docs/installation/",children:"installation\nguide"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"import json\nimport os\nimport random\nimport time\nfrom typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport requests\nfrom PIL import Image\nfrom termcolor import colored\n\nimport autogen\nfrom autogen import Agent, AssistantAgent, ConversableAgent, UserProxyAgent\nfrom autogen.agentchat.contrib.capabilities.vision_capability import VisionCapability\nfrom autogen.agentchat.contrib.img_utils import get_pil_image, pil_to_data_uri\nfrom autogen.agentchat.contrib.multimodal_conversable_agent import MultimodalConversableAgent\nfrom autogen.code_utils import content_str\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)("a",{id:"app-1"})," ## Application 1: Image Chat"]}),"\n",(0,a.jsx)(n.p,{children:"In this section, we present a straightforward dual-agent architecture to\nenable user to chat with a multimodal agent."}),"\n",(0,a.jsxs)(n.p,{children:["First, we show this image and ask a question.\n",(0,a.jsx)(n.img,{src:"https://th.bing.com/th/id/R.422068ce8af4e15b0634fe2540adea7a?rik=y4OcXBE%2fqutDOw&pid=ImgRaw&r=0",alt:""})]}),"\n",(0,a.jsx)(n.p,{children:"Within the user proxy agent, we can decide to activate the human input\nmode or not (for here, we use human_input_mode=\u201cNEVER\u201d for conciseness).\nThis allows you to interact with LMM in a multi-round dialogue, enabling\nyou to provide feedback as the conversation unfolds."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'config_list_4v = autogen.config_list_from_json(\n    "OAI_CONFIG_LIST",\n    filter_dict={\n        "model": ["gpt-4-vision-preview"],\n    },\n)\n\n\nconfig_list_gpt4 = autogen.config_list_from_json(\n    "OAI_CONFIG_LIST",\n    filter_dict={\n        "model": ["gpt-4", "gpt-4-0314", "gpt4", "gpt-4-32k", "gpt-4-32k-0314", "gpt-4-32k-v0314"],\n    },\n)\n\ngpt4_llm_config = {"config_list": config_list_gpt4, "cache_seed": 42}\n'})}),"\n",(0,a.jsxs)(n.p,{children:["Learn more about configuring LLMs for agents\n",(0,a.jsx)(n.a,{href:"../../docs/topics/llm_configuration",children:"here"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'image_agent = MultimodalConversableAgent(\n    name="image-explainer",\n    max_consecutive_auto_reply=10,\n    llm_config={"config_list": config_list_4v, "temperature": 0.5, "max_tokens": 300},\n)\n\nuser_proxy = autogen.UserProxyAgent(\n    name="User_proxy",\n    system_message="A human admin.",\n    human_input_mode="NEVER",  # Try between ALWAYS or NEVER\n    max_consecutive_auto_reply=0,\n    code_execution_config={\n        "use_docker": False\n    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n)\n\n# Ask the question with an image\nuser_proxy.initiate_chat(\n    image_agent,\n    message="""What\'s the breed of this dog?\n<img https://th.bing.com/th/id/R.422068ce8af4e15b0634fe2540adea7a?rik=y4OcXBE%2fqutDOw&pid=ImgRaw&r=0>.""",\n)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"User_proxy (to image-explainer):\n\nWhat's the breed of this dog?\n<image>.\n\n--------------------------------------------------------------------------------\n\n>>>>>>>> USING AUTO REPLY...\nimage-explainer (to User_proxy):\n\nThe dog in the image appears to be a Goldendoodle, which is a crossbreed between a Golden Retriever and a Poodle. They are known for their curly, hypoallergenic coats, which can vary in color, and their friendly and affectionate nature.\n\n--------------------------------------------------------------------------------\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"/home/beibinli/autogen/autogen/agentchat/conversable_agent.py:1121: UserWarning: Cannot extract summary using last_msg: 'list' object has no attribute 'replace'. Using an empty str as summary.\n  warnings.warn(f\"Cannot extract summary using last_msg: {e}. Using an empty str as summary.\", UserWarning)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"ChatResult(chat_id=None, chat_history=[{'content': \"What's the breed of this dog?\\n<img https://th.bing.com/th/id/R.422068ce8af4e15b0634fe2540adea7a?rik=y4OcXBE%2fqutDOw&pid=ImgRaw&r=0>.\", 'role': 'assistant'}, {'content': 'The dog in the image appears to be a Goldendoodle, which is a crossbreed between a Golden Retriever and a Poodle. They are known for their curly, hypoallergenic coats, which can vary in color, and their friendly and affectionate nature.', 'role': 'user'}], summary='', cost=({'total_cost': 0.013030000000000002, 'gpt-4-1106-vision-preview': {'cost': 0.013030000000000002, 'prompt_tokens': 1132, 'completion_tokens': 57, 'total_tokens': 1189}}, {'total_cost': 0}), human_input=[])\n"})}),"\n",(0,a.jsx)(n.p,{children:"Now, input another image, and ask a followup question."}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:"https://th.bing.com/th/id/OIP.29Mi2kJmcHHyQVGe_0NG7QHaEo?pid=ImgDet&rs=1",alt:""})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Ask the question with an image\nuser_proxy.send(\n    message="""What is this breed?\n<img https://th.bing.com/th/id/OIP.29Mi2kJmcHHyQVGe_0NG7QHaEo?pid=ImgDet&rs=1>\n\nAmong the breeds, which one barks less?""",\n    recipient=image_agent,\n)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"User_proxy (to image-explainer):\n\nWhat is this breed?\n<image>\n\nAmong the breeds, which one barks less?\n\n--------------------------------------------------------------------------------\n\n>>>>>>>> USING AUTO REPLY...\nimage-explainer (to User_proxy):\n\nThe dog in the image is a Siberian Husky. They are known for their striking appearance, with thick fur, erect triangular ears, and distinctive markings.\n\nWhen comparing the barking tendencies of Siberian Huskies and Goldendoodles, Huskies are generally known to bark less. Siberian Huskies are more prone to howling and vocalizing in other ways rather than barking. Goldendoodles can vary in their tendency to bark based on the traits they inherit from their Golden Retriever and Poodle parents. However, every dog is an individual, and their environment, training, and socialization can greatly influence their tendency to bark.\n\n--------------------------------------------------------------------------------\n"})}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)("a",{id:"app-2"})," ## Application 2: Figure Creator"]}),"\n",(0,a.jsxs)(n.p,{children:["Here, we define a ",(0,a.jsx)(n.code,{children:"FigureCreator"})," agent, which contains three child\nagents: commander, coder, and critics."]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Commander: interacts with users, runs code, and coordinates the flow\nbetween the coder and critics."}),"\n",(0,a.jsx)(n.li,{children:"Coder: writes code for visualization."}),"\n",(0,a.jsx)(n.li,{children:"Critics: LMM-based agent that provides comments and feedback on the\ngenerated image."}),"\n"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'working_dir = "tmp/"\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'class FigureCreator(ConversableAgent):\n    def __init__(self, n_iters=2, **kwargs):\n        """\n        Initializes a FigureCreator instance.\n\n        This agent facilitates the creation of visualizations through a collaborative effort among its child agents: commander, coder, and critics.\n\n        Parameters:\n            - n_iters (int, optional): The number of "improvement" iterations to run. Defaults to 2.\n            - **kwargs: keyword arguments for the parent AssistantAgent.\n        """\n        super().__init__(**kwargs)\n        self.register_reply([Agent, None], reply_func=FigureCreator._reply_user, position=0)\n        self._n_iters = n_iters\n\n    def _reply_user(self, messages=None, sender=None, config=None):\n        if all((messages is None, sender is None)):\n            error_msg = f"Either {messages=} or {sender=} must be provided."\n            logger.error(error_msg)  # noqa: F821\n            raise AssertionError(error_msg)\n        if messages is None:\n            messages = self._oai_messages[sender]\n\n        user_question = messages[-1]["content"]\n\n        ### Define the agents\n        commander = AssistantAgent(\n            name="Commander",\n            human_input_mode="NEVER",\n            max_consecutive_auto_reply=10,\n            system_message="Help me run the code, and tell other agents it is in the <img result.jpg> file location.",\n            is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),\n            code_execution_config={"last_n_messages": 3, "work_dir": working_dir, "use_docker": False},\n            llm_config=self.llm_config,\n        )\n\n        critics = MultimodalConversableAgent(\n            name="Critics",\n            system_message="""Criticize the input figure. How to replot the figure so it will be better? Find bugs and issues for the figure.\n            Pay attention to the color, format, and presentation. Keep in mind of the reader-friendliness.\n            If you think the figures is good enough, then simply say NO_ISSUES""",\n            llm_config={"config_list": config_list_4v, "max_tokens": 300},\n            human_input_mode="NEVER",\n            max_consecutive_auto_reply=1,\n            #     use_docker=False,\n        )\n\n        coder = AssistantAgent(\n            name="Coder",\n            llm_config=self.llm_config,\n        )\n\n        coder.update_system_message(\n            coder.system_message\n            + "ALWAYS save the figure in `result.jpg` file. Tell other agents it is in the <img result.jpg> file location."\n        )\n\n        # Data flow begins\n        commander.initiate_chat(coder, message=user_question)\n        img = Image.open(os.path.join(working_dir, "result.jpg"))\n        plt.imshow(img)\n        plt.axis("off")  # Hide the axes\n        plt.show()\n\n        for i in range(self._n_iters):\n            commander.send(\n                message=f"Improve <img {os.path.join(working_dir, \'result.jpg\')}>",\n                recipient=critics,\n                request_reply=True,\n            )\n\n            feedback = commander._oai_messages[critics][-1]["content"]\n            if feedback.find("NO_ISSUES") >= 0:\n                break\n            commander.send(\n                message="Here is the feedback to your figure. Please improve! Save the result to `result.jpg`\\n"\n                + feedback,\n                recipient=coder,\n                request_reply=True,\n            )\n            img = Image.open(os.path.join(working_dir, "result.jpg"))\n            plt.imshow(img)\n            plt.axis("off")  # Hide the axes\n            plt.show()\n\n        return True, os.path.join(working_dir, "result.jpg")\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'creator = FigureCreator(name="Figure Creator~", llm_config=gpt4_llm_config)\n\nuser_proxy = autogen.UserProxyAgent(\n    name="User", human_input_mode="NEVER", max_consecutive_auto_reply=0, code_execution_config={"use_docker": False}\n)\n\nuser_proxy.initiate_chat(\n    creator,\n    message="""\nPlot a figure by using the data from:\nhttps://raw.githubusercontent.com/vega/vega/main/docs/data/seattle-weather.csv\n\nI want to show both temperature high and low.\n""",\n)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"User (to Figure Creator~):\n\n\nPlot a figure by using the data from:\nhttps://raw.githubusercontent.com/vega/vega/main/docs/data/seattle-weather.csv\n\nI want to show both temperature high and low.\n\n\n--------------------------------------------------------------------------------\nCommander (to Coder):\n\n\nPlot a figure by using the data from:\nhttps://raw.githubusercontent.com/vega/vega/main/docs/data/seattle-weather.csv\n\nI want to show both temperature high and low.\n\n\n--------------------------------------------------------------------------------\nCoder (to Commander):\n\nSure, let's first download the CSV data from the provided URL using Python and then we'll plot the figure with both temperature high and low.\n\nTo start with, I'll provide you with a Python script that will download the CSV file and then plot the temperatures highs and lows.\n\nPlease run this Python code:\n\n```python\n# filename: plot_temperatures.py\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# URL to download the CSV data\nurl = \"https://raw.githubusercontent.com/vega/vega/main/docs/data/seattle-weather.csv\"\n\n# Download the data\ndata = pd.read_csv(url)\n\n# Now, let us plot the high and low temperatures\nplt.figure(figsize=(10, 5))\nplt.plot(data['date'], data['temp_max'], label='High Temp')\nplt.plot(data['date'], data['temp_min'], label='Low Temp')\n\n# Providing labels and title\nplt.xlabel('Date')\nplt.ylabel('Temperature (\xb0C)')\nplt.title('High and Low Temperatures in Seattle')\nplt.legend()\n\n# Rotate the dates on x-axis for better readability\nplt.xticks(rotation=45)\nplt.tight_layout()\n\n# Save the figure\nplt.savefig('result.jpg')\n\nprint(\"The plot has been saved as 'result.jpg'.\")\n```\n\nMake sure you have the required packages (`pandas` and `matplotlib`) installed. If they are not installed, you can install them using `pip`:\n\n```sh\npip install pandas matplotlib\n```\n\nAfter running the script, you should see a message indicating that the plot has been saved as 'result.jpg'.\n\n--------------------------------------------------------------------------------\n\n>>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\n\n>>>>>>>> EXECUTING CODE BLOCK 1 (inferred language is sh)...\nCommander (to Coder):\n\nexitcode: 0 (execution succeeded)\nCode output: \nThe plot has been saved as 'result.jpg'.\n\nRequirement already satisfied: pandas in /home/beibinli/anaconda3/lib/python3.9/site-packages (1.5.3)\nRequirement already satisfied: matplotlib in /home/beibinli/anaconda3/lib/python3.9/site-packages (3.7.1)\nRequirement already satisfied: python-dateutil>=2.8.1 in /home/beibinli/anaconda3/lib/python3.9/site-packages (from pandas) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /home/beibinli/anaconda3/lib/python3.9/site-packages (from pandas) (2022.7.1)\nRequirement already satisfied: numpy>=1.20.3 in /home/beibinli/anaconda3/lib/python3.9/site-packages (from pandas) (1.23.5)\nRequirement already satisfied: contourpy>=1.0.1 in /home/beibinli/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.0.5)\nRequirement already satisfied: cycler>=0.10 in /home/beibinli/anaconda3/lib/python3.9/site-packages (from matplotlib) (0.11.0)\nRequirement already satisfied: fonttools>=4.22.0 in /home/beibinli/anaconda3/lib/python3.9/site-packages (from matplotlib) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /home/beibinli/anaconda3/lib/python3.9/site-packages (from matplotlib) (1.4.4)\nRequirement already satisfied: packaging>=20.0 in /home/beibinli/anaconda3/lib/python3.9/site-packages (from matplotlib) (23.2)\nRequirement already satisfied: pillow>=6.2.0 in /home/beibinli/anaconda3/lib/python3.9/site-packages (from matplotlib) (10.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /home/beibinli/anaconda3/lib/python3.9/site-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: importlib-resources>=3.2.0 in /home/beibinli/anaconda3/lib/python3.9/site-packages (from matplotlib) (5.13.0)\nRequirement already satisfied: zipp>=3.1.0 in /home/beibinli/anaconda3/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.11.0)\nRequirement already satisfied: six>=1.5 in /home/beibinli/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n\n\n--------------------------------------------------------------------------------\nCoder (to Commander):\n\nThe Python script executed successfully, and the plot has been saved as 'result.jpg'. You should now be able to find the figure file at the mentioned location. \n\nThe necessary packages were already installed in your environment, as indicated by your output, so no further action was required there.\n\nThe figure shows the high and low temperatures for Seattle as indicated by the data in the CSV file you provided. If you have any other requests or need further assistance, feel free to ask. If not, and if you are satisfied with the result, we can conclude this task.\n\nTERMINATE\n\n--------------------------------------------------------------------------------\nCommander (to Critics):\n\nImprove <image>\n\n--------------------------------------------------------------------------------\nCritics (to Commander):\n\nThe plot depicting high and low temperatures in Seattle can be improved in several ways:\n\n1. Date Axis: The dates on the x-axis are not visible or readable, which makes it hard to associate the temperature data with specific time periods. Consider formatting the date axis so that the dates are displayed at a readable angle or with sufficient spacing.\n\n2. X-axis Labeling: There appears to be an issue with the x-axis labels or ticks as it shows a dense black area, indicating overlapping or condensed labels. The frequency and format of the labels should be adjusted to prevent overlap and allow for clear reading.\n\n3. Y-axis Range: Consider starting the y-axis at a lower value if the temperature data permits, to give more space to the temperature lines and improve readability.\n\n4. Gridlines: Adding horizontal gridlines can aid in reading specific temperatures at a glance.\n\n5. Legend: The legend clearly indicates which color corresponds to high and low temperatures, which is good. Ensure the legend does not obscure any important data.\n\n6. Resolution: The actual resolution of the image may be fine, but the x-axis issue might give the illusion of a low-resolution image. Once the x-axis is corrected, the overall clarity of the chart may naturally improve.\n\n7. Color Contrast: The blue and orange color choices are generally good for contrast, but ensure they are differentiable to all users, including those who may be colorblind. Consider using colorblind-friendly palettes.\n\n8. Title: The title \"High\n\n--------------------------------------------------------------------------------\nCommander (to Coder):\n\nHere is the feedback to your figure. Please improve! Save the result to `result.jpg`\nThe plot depicting high and low temperatures in Seattle can be improved in several ways:\n\n1. Date Axis: The dates on the x-axis are not visible or readable, which makes it hard to associate the temperature data with specific time periods. Consider formatting the date axis so that the dates are displayed at a readable angle or with sufficient spacing.\n\n2. X-axis Labeling: There appears to be an issue with the x-axis labels or ticks as it shows a dense black area, indicating overlapping or condensed labels. The frequency and format of the labels should be adjusted to prevent overlap and allow for clear reading.\n\n3. Y-axis Range: Consider starting the y-axis at a lower value if the temperature data permits, to give more space to the temperature lines and improve readability.\n\n4. Gridlines: Adding horizontal gridlines can aid in reading specific temperatures at a glance.\n\n5. Legend: The legend clearly indicates which color corresponds to high and low temperatures, which is good. Ensure the legend does not obscure any important data.\n\n6. Resolution: The actual resolution of the image may be fine, but the x-axis issue might give the illusion of a low-resolution image. Once the x-axis is corrected, the overall clarity of the chart may naturally improve.\n\n7. Color Contrast: The blue and orange color choices are generally good for contrast, but ensure they are differentiable to all users, including those who may be colorblind. Consider using colorblind-friendly palettes.\n\n8. Title: The title \"High\n\n--------------------------------------------------------------------------------\nCoder (to Commander):\n\nTo address the feedback received on the figure, we'll make the following improvements to the plot:\n\n1. Format the date axis for better readability.\n2. Adjust the frequency and format of the x-axis labels to prevent overlap.\n3. Extend the y-axis range if possible.\n4. Add horizontal gridlines for ease of reading temperatures.\n5. Ensure the legend is placed properly to avoid obscuring data.\n6. Check for clarity once x-axis labels are resolved.\n7. Use a colorblind-friendly palette for the plot.\n8. Ensure the title is descriptive and relevant.\n\nHere's an updated Python script to make these improvements:\n\n```python\n# filename: improved_plot_temperatures.py\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\n\n# URL to download the CSV data\nurl = \"https://raw.githubusercontent.com/vega/vega/main/docs/data/seattle-weather.csv\"\n\n# Download the data\ndata = pd.read_csv(url)\ndata['date'] = pd.to_datetime(data['date'])\n\n# Now, let us plot the high and low temperatures\nplt.figure(figsize=(10, 5))\nplt.plot(data['date'], data['temp_max'], label='High Temp', color='tab:blue')\nplt.plot(data['date'], data['temp_min'], label='Low Temp', color='tab:orange')\n\n# Set the x-axis major locator and formatter for better date display\nplt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\nplt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n\n# Rotate the dates on x-axis for better readability\nplt.gcf().autofmt_xdate()\n\n# Improving Y-axis Range\ntemp_min = data['temp_min'].min() - 5  # Give some padding below the min temp\ntemp_max = data['temp_max'].max() + 5  # Give some padding above the max temp\nplt.ylim(temp_min, temp_max)\n\n# Adding gridlines and formatting the grid\nplt.grid(True, which='both', linestyle='--', linewidth=0.5)\n\n# Providing labels and title\nplt.xlabel('Date')\nplt.ylabel('Temperature (\xb0C)')\nplt.title('High and Low Temperatures in Seattle Over Time')\n\n# Adjusting legend position\nplt.legend(loc='upper right')\n\n# Check done, no need to adjust color contrast as colorblind-friendly colors are used\n\n# Save the figure in high resolution\nplt.savefig('result.jpg', dpi=300)\n\nprint(\"The improved plot has been saved as 'result.jpg'.\")\n```\n\nPlease execute this script. Make sure that the `matplotlib.dates` module is available in your environment before running the script. If you encounter any issues, let me know.\n\n--------------------------------------------------------------------------------\n\n>>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\nCommander (to Coder):\n\nexitcode: 0 (execution succeeded)\nCode output: \nThe improved plot has been saved as 'result.jpg'.\n\n\n--------------------------------------------------------------------------------\nCoder (to Commander):\n\nThe improved plot has been saved successfully as 'result.jpg'. The improvements should address the feedback points provided earlier, resulting in a clearer and more readable figure.\n\nThe dates on the x-axis are now formatted for better visibility, the frequency of x-axis labels has been adjusted, the y-axis range has been extended, gridlines have been added, and the legend has been positioned properly. The colors used should be distinguishable by individuals with color vision deficiencies.\n\nIf no further improvements are needed, the task is now complete.\n\nTERMINATE\n\n--------------------------------------------------------------------------------\nCommander (to Critics):\n\nImprove <image>\n\n--------------------------------------------------------------------------------\nCommander (to Coder):\n\nHere is the feedback to your figure. Please improve! Save the result to `result.jpg`\nImprove <img tmp/result.jpg>\n\n--------------------------------------------------------------------------------\nCoder (to Commander):\n\nI apologize for any confusion, but as an AI, I don't have the capability to directly view the images generated. However, I can certainly help you further refine the plot based on standard practices if there is anything specific that you feel needs further adjustment.\n\nYou mentioned feedback for a figure referenced as \"<img tmp/result.jpg>\". Since I can't view images, I'm assuming this feedback is a placeholder. In case there are specific changes or additional improvements you want to implement in the figure, please provide me with those details explicitly.\n\nFor instance:\n- Are there any specific elements in the legend that still need repositioning?\n- Should the date formatting be adjusted further for better clarity?\n- Are there any other specific elements (e.g. colors, lines, labels, title) that require modification?\n\nWithout specific feedback, I can suggest some general improvements, such as:\n\n- Ensuring the plot does not appear cluttered.\n- Checking if the aspect ratio is appropriate for the data being presented.\n- Verifying that the font sizes are readable.\n\nIf specific feedback can be provided, we can proceed accordingly. Otherwise, please let me know if we should conclude the task.\n\nTERMINATE\n\n--------------------------------------------------------------------------------\nFigure Creator~ (to User):\n\ntmp/result.jpg\n\n--------------------------------------------------------------------------------\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:t(60579).Z+"",width:"515",height:"268"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:t(63539).Z+"",width:"515",height:"268"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.img,{src:t(34904).Z+"",width:"515",height:"268"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"ChatResult(chat_id=None, chat_history=[{'content': '\\nPlot a figure by using the data from:\\nhttps://raw.githubusercontent.com/vega/vega/main/docs/data/seattle-weather.csv\\n\\nI want to show both temperature high and low.\\n', 'role': 'assistant'}, {'content': 'tmp/result.jpg', 'role': 'user'}], summary='tmp/result.jpg', cost=({'total_cost': 0}, {'total_cost': 0}), human_input=[])\n"})}),"\n",(0,a.jsx)(n.h2,{id:"vision-capability-group-chat-example-with-multimodal-agent",children:"Vision Capability: Group Chat Example with Multimodal Agent"}),"\n",(0,a.jsx)(n.p,{children:"We recommend using VisionCapability for group chat managers so that it\ncan organize and understand images better."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'agent1 = MultimodalConversableAgent(\n    name="image-explainer-1",\n    max_consecutive_auto_reply=10,\n    llm_config={"config_list": config_list_4v, "temperature": 0.5, "max_tokens": 300},\n    system_message="Your image description is poetic and engaging.",\n)\nagent2 = MultimodalConversableAgent(\n    name="image-explainer-2",\n    max_consecutive_auto_reply=10,\n    llm_config={"config_list": config_list_4v, "temperature": 0.5, "max_tokens": 300},\n    system_message="Your image description is factual and to the point.",\n)\n\nuser_proxy = autogen.UserProxyAgent(\n    name="User_proxy",\n    system_message="Desribe image for me.",\n    human_input_mode="TERMINATE",  # Try between ALWAYS, NEVER, and TERMINATE\n    max_consecutive_auto_reply=10,\n    code_execution_config={\n        "use_docker": False\n    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n)\n\n# We set max_round to 5\ngroupchat = autogen.GroupChat(agents=[agent1, agent2, user_proxy], messages=[], max_round=5)\n\nvision_capability = VisionCapability(lmm_config={"config_list": config_list_4v, "temperature": 0.5, "max_tokens": 300})\ngroup_chat_manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=gpt4_llm_config)\nvision_capability.add_to_agent(group_chat_manager)\n\nrst = user_proxy.initiate_chat(\n    group_chat_manager,\n    message="""Write a poet for my image:\n                        <img https://th.bing.com/th/id/R.422068ce8af4e15b0634fe2540adea7a?rik=y4OcXBE%2fqutDOw&pid=ImgRaw&r=0>.""",\n)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"User_proxy (to chat_manager):\n\nWrite a poet for my image:\n                        <img https://th.bing.com/th/id/R.422068ce8af4e15b0634fe2540adea7a?rik=y4OcXBE%2fqutDOw&pid=ImgRaw&r=0>.\n\n--------------------------------------------------------------------------------\n\n>>>>>>>> USING AUTO REPLY...\nimage-explainer-1 (to chat_manager):\n\nIn a cozy nook by the door, where whispers of adventure pour,\nLies a curly apricot dream, with eyes that softly gleam.\nWestphal, a name upon his tag, a dapper bow, no room to brag.\n\nA poodle or a mix so sweet, with paws that barely seem to meet\nThe floor that holds his tender frame, in this quiet household game.\nA sentinel of love and joy, more charming than any toy.\n\nHis gaze, a tale of gentle days, and love that in his heart blazes.\nWith boots behind, like silent guards, he watches without regards\nTo the world's rush and its tides, in his serene abode, he abides.\n\nOh, little guardian of the hearth, your worth is not defined by birth.\nBut by the joy you bring to life, a respite from our daily strife.\nIn you, we find a friend so true, Westphal, a treasure through and through.\n\n--------------------------------------------------------------------------------\n\n>>>>>>>> USING AUTO REPLY...\nimage-explainer-2 (to chat_manager):\n\nA pup so young with fur of gold,\nIn a house where stories unfold.\nWestphal, with your bow so neat,\nYour puppy eyes, oh so sweet.\n\nCurled up on the floor so light,\nYour coat a soft, delightful sight.\nA mix, perhaps, of breeds so kind,\nIn your face, pure joy we find.\n\nWith a tag that jingles slight,\nAnd a bow tie oh so bright,\nYou lie there with such grace,\nA calm presence in this place.\n\nBehind you, boots await the rain,\nBut inside, you contently remain.\nA symbol of love, a furry delight,\nA beacon of warmth, shining so bright.\n\nIn a world that's fast and vast,\nYour gentle spirit holds us fast.\nWestphal, with your charm so grand,\nYou bring peace to this human land.\n\n--------------------------------------------------------------------------------\n\n>>>>>>>> USING AUTO REPLY...\nUser_proxy (to chat_manager):\n\n\n\n--------------------------------------------------------------------------------\n\n>>>>>>>> USING AUTO REPLY...\nUser_proxy (to chat_manager):\n\n\n\n--------------------------------------------------------------------------------\n"})}),"\n",(0,a.jsx)(n.h2,{id:"behavior-with-and-without-visioncapability-for-agents",children:"Behavior with and without VisionCapability for Agents"}),"\n",(0,a.jsx)(n.p,{children:"Here, we show the behavior of an agent with and without\nVisionCapability. We use the same image and question as in the previous\nexample."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'agent_no_vision = AssistantAgent(name="Regular LLM Agent", llm_config=gpt4_llm_config)\n\nagent_with_vision = AssistantAgent(name="Regular LLM Agent with Vision Capability", llm_config=gpt4_llm_config)\nvision_capability = VisionCapability(lmm_config={"config_list": config_list_4v, "temperature": 0.5, "max_tokens": 300})\nvision_capability.add_to_agent(agent_with_vision)\n\n\nuser = UserProxyAgent(\n    name="User",\n    human_input_mode="NEVER",\n    max_consecutive_auto_reply=0,\n    code_execution_config={"use_docker": False},\n)\n\nmessage = """Write a poet for my image:\n                        <img https://th.bing.com/th/id/R.422068ce8af4e15b0634fe2540adea7a?rik=y4OcXBE%2fqutDOw&pid=ImgRaw&r=0>."""\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"user.send(message=message, recipient=agent_no_vision, request_reply=True)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"User (to Regular LLM Agent):\n\nWrite a poet for my image:\n                        <img https://th.bing.com/th/id/R.422068ce8af4e15b0634fe2540adea7a?rik=y4OcXBE%2fqutDOw&pid=ImgRaw&r=0>.\n\n--------------------------------------------------------------------------------\nRegular LLM Agent (to User):\n\nAs an AI, I can't directly view images or web content. However, I can help you generate a poem by gathering information about the image. Please describe the image for me, including details such as the setting, prominent colors, the mood it evokes, and any specific elements like animals, nature, cityscapes, or people that you want to be highlighted in the poem. Once you provide a description of the image, I can compose a poem based on that description.\n\n--------------------------------------------------------------------------------\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"user.send(message=message, recipient=agent_with_vision, request_reply=True)\n"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"User (to Regular LLM Agent with Vision Capability):\n\nWrite a poet for my image:\n                        <img https://th.bing.com/th/id/R.422068ce8af4e15b0634fe2540adea7a?rik=y4OcXBE%2fqutDOw&pid=ImgRaw&r=0>.\n\n--------------------------------------------------------------------------------\nRegular LLM Agent with Vision Capability (to User):\n\nIn apricot hues, a curl-coated pup reclines,\nWith Westphal's name upon his tag that shines.\nA bow tie blooms in a brilliant blue sky,\nAmid a light-drenched floor where soft paws lie.\n\nPossessor of gazes, tender and deep,\nIn the quiet foyer, his watch he keeps.\nBeneath black rubber guards of rainy days,\nHe stirs a comfort, a homely embrace.\n\nHis lineage drawn from the poodles' grace,\nOr maybe a mix, with a doodle's face,\nGold or Lab, his curls are just as sweet,\nIn each bouncing step, in each heartbeat.\n\nA picture of love, in a tiny frame,\nA heartbeat wrapped in an apricot mane.\nThe pup in his calm, an invite to cheer,\nA whisper of joy in a pet-lover's ear.\n\nAround him, life's simple clutter does unfold,\nYet untouched by worry, untouched by cold.\nWith every breath, he claims this slice of earth,\nA master of mirth, from the moment of his birth.\n\nPaws outstretched on the soft, forgiving ground,\nHis soulful eyes speak, without a sound.\nFor in the sweet stillness of his gentle rest,\nLies the simple truth that we are blessed.\n\nTERMINATE\n\n--------------------------------------------------------------------------------\n"})}),"\n",(0,a.jsx)(n.h2,{id:"custom-caption-function-for-vision-capability",children:"Custom Caption Function for Vision Capability"}),"\n",(0,a.jsx)(n.p,{children:"In many use cases, we can use a custom function within the Vision\nCapability to transcribe an image into a caption."}),"\n",(0,a.jsx)(n.p,{children:"For instance, we can use rule-based algorithm or other models to detect\nthe color, box, and other components inside the image."}),"\n",(0,a.jsx)(n.p,{children:"The custom model should take a path to the image and return a string\ncaption."}),"\n",(0,a.jsx)(n.p,{children:"In the example below, the Vision Capability will call LMM to get caption\nand also call the custom function to get more information."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'def my_description(image_url: str, image_data: Image = None, lmm_client: object = None) -> str:\n    """\n    This function takes an image URL and returns the description.\n\n    Parameters:\n        - image_url (str): The URL of the image.\n        - image_data (PIL.Image): The image data.\n        - lmm_client (object): The LLM client object.\n\n    Returns:\n        - str: A description of the color of the image.\n    """\n    # Print the arguments for illustration purpose\n    print("image_url", image_url)\n    print("image_data", image_data)\n    print("lmm_client", lmm_client)\n\n    img_uri = pil_to_data_uri(image_data)  # cast data into URI (str) format for API call\n    lmm_out = lmm_client.create(\n        context=None,\n        messages=[\n            {\n                "role": "user",\n                "content": [\n                    {"type": "text", "text": "Describe this image in 10 words."},\n                    {\n                        "type": "image_url",\n                        "image_url": {\n                            "url": img_uri,\n                        },\n                    },\n                ],\n            }\n        ],\n    )\n    description = lmm_out.choices[0].message.content\n    description = content_str(description)\n\n    # Convert the image into an array of pixels.\n    pixels = np.array(image_data)\n\n    # Calculate the average color.\n    avg_color_per_row = np.mean(pixels, axis=0)\n    avg_color = np.mean(avg_color_per_row, axis=0)\n    avg_color = avg_color.astype(int)  # Convert to integer for color values\n\n    # Format the average color as a string description.\n    caption = f"""The image is from {image_url}\n    It is about: {description}\n    The average color of the image is RGB:\n        ({avg_color[0]}, {avg_color[1]}, {avg_color[2]})"""\n\n    print(caption)  # For illustration purpose\n\n    return caption\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'agent_with_vision_and_func = AssistantAgent(\n    name="Regular LLM Agent with Custom Func and LMM", llm_config=gpt4_llm_config\n)\n\nvision_capability_with_func = VisionCapability(\n    lmm_config={"config_list": config_list_4v, "temperature": 0.5, "max_tokens": 300},\n    custom_caption_func=my_description,\n)\nvision_capability_with_func.add_to_agent(agent_with_vision_and_func)\n\nuser.send(message=message, recipient=agent_with_vision_and_func, request_reply=True)\n'})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-text",children:"User (to Regular LLM Agent with Custom Func and LMM):\n\nWrite a poet for my image:\n                        <img https://th.bing.com/th/id/R.422068ce8af4e15b0634fe2540adea7a?rik=y4OcXBE%2fqutDOw&pid=ImgRaw&r=0>.\n\n--------------------------------------------------------------------------------\nimage_url https://th.bing.com/th/id/R.422068ce8af4e15b0634fe2540adea7a?rik=y4OcXBE%2fqutDOw&pid=ImgRaw&r=0\nimage_data <PIL.Image.Image image mode=RGB size=1920x1080 at 0x7F599DA4CCA0>\nlmm_client <autogen.oai.client.OpenAIWrapper object at 0x7f599da3ab20>\nThe image is from https://th.bing.com/th/id/R.422068ce8af4e15b0634fe2540adea7a?rik=y4OcXBE%2fqutDOw&pid=ImgRaw&r=0\n    It is about: Cute brown curly-haired puppy with blue collar indoors.\n    The average color of the image is RGB: \n        (170, 155, 137)\nRegular LLM Agent with Custom Func and LMM (to User):\n\nBeneath a sky of homely hue,\nWhere RGB blends a gentle stew,\nLies a pup of curls and capers,\nBound by blue, his neck in drapers.\n\nSoft in gaze, his eyes implore,\nWarming hearts down to the core,\nCoat of brown, with tangles sweet,\nWhispers of play in each petite feet.\n\nIn a world quite vast and wide,\nIndoors he sits, with pride inside.\nA silent wish, a breath, a start,\nCurly companion, a work of art.\n\nWithin the frame, he's captured still,\nYet, through the lens, his charm does spill.\nA tiny heartbeat in the quiet room,\nHis presence banishes all gloom.\n\nA puppy's joy, in sepia tone,\nWith collar blue, he reigns alone.\nA picture's worth, this moment's glee,\nCute curly friend, for all to see.\n\nThis poem encapsulates the essence of the cute brown curly-haired puppy wearing a blue collar, blending an emotional portrayal with the aesthetic elements you provided.\n\n--------------------------------------------------------------------------------\n"})})]})}function c(e={}){const{wrapper:n}={...(0,i.a)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(h,{...e})}):h(e)}},60579:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/cell-8-output-2-14c00428a65418a4a13fcbd8bf5d0e23.png"},63539:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/cell-8-output-3-f63929642a0b3f1aa15418a297dc9886.png"},34904:(e,n,t)=>{t.d(n,{Z:()=>a});const a=t.p+"assets/images/cell-8-output-4-f63929642a0b3f1aa15418a297dc9886.png"},11151:(e,n,t)=>{t.d(n,{Z:()=>r,a:()=>s});var a=t(67294);const i={},o=a.createContext(i);function s(e){const n=a.useContext(o);return a.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),a.createElement(o.Provider,{value:n},e.children)}}}]);