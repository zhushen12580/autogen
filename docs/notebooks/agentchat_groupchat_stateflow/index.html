<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-notebooks/agentchat_groupchat_stateflow" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.1">
<title data-rh="true">StateFlow: Build Workflows through State-Oriented Actions | AutoGen</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://microsoft.github.io/autogen/docs/notebooks/agentchat_groupchat_stateflow"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="StateFlow: Build Workflows through State-Oriented Actions | AutoGen"><meta data-rh="true" name="description" content="StateFlow: Build Workflows through State-Oriented Actions"><meta data-rh="true" property="og:description" content="StateFlow: Build Workflows through State-Oriented Actions"><link data-rh="true" rel="icon" href="/autogen/img/ag.ico"><link data-rh="true" rel="canonical" href="https://microsoft.github.io/autogen/docs/notebooks/agentchat_groupchat_stateflow"><link data-rh="true" rel="alternate" href="https://microsoft.github.io/autogen/docs/notebooks/agentchat_groupchat_stateflow" hreflang="en"><link data-rh="true" rel="alternate" href="https://microsoft.github.io/autogen/docs/notebooks/agentchat_groupchat_stateflow" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/autogen/blog/rss.xml" title="AutoGen RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/autogen/blog/atom.xml" title="AutoGen Atom Feed">






<script>!function(e,t,n,c,s,a,r){e[n]=e[n]||function(){(e[n].q=e[n].q||[]).push(arguments)},(a=t.createElement(c)).async=1,a.src="https://www.clarity.ms/tag/lnxpe6skj1",(r=t.getElementsByTagName(c)[0]).parentNode.insertBefore(a,r)}(window,document,"clarity","script")</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
<script src="/autogen/js/custom.js" async defer="defer"></script><link rel="stylesheet" href="/autogen/assets/css/styles.d8ef5d37.css">
<script src="/autogen/assets/js/runtime~main.33d2ca05.js" defer="defer"></script>
<script src="/autogen/assets/js/main.f5c79556.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const a=new URLSearchParams(window.location.search).entries();for(var[t,e]of a)if(t.startsWith("docusaurus-data-")){var n=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(n,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="announcementBar_mb4j" style="background-color:#fafbfc;color:#091E42" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY">What's new in AutoGen? Read <a href="/autogen/blog/2024/03/03/AutoGen-Update">this blog</a> for an overview of updates</div><button type="button" aria-label="Close" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/autogen/"><div class="navbar__logo"><img src="/autogen/img/ag.svg" alt="AutoGen" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/autogen/img/ag.svg" alt="AutoGen" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AutoGen</b></a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Docs</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/autogen/docs/Getting-Started">Getting Started</a></li><li><a class="dropdown__link" href="/autogen/docs/installation/">Installation</a></li><li><a class="dropdown__link" href="/autogen/docs/tutorial/introduction">Tutorial</a></li><li><a class="dropdown__link" href="/autogen/docs/topics">User Guide</a></li><li><a class="dropdown__link" href="/autogen/docs/reference/agentchat/conversable_agent">API Reference</a></li><li><a class="dropdown__link" href="/autogen/docs/FAQ">FAQs</a></li><li><a class="dropdown__link" href="/autogen/docs/autogen-studio/getting-started">AutoGen Studio</a></li><li><a class="dropdown__link" href="/autogen/docs/ecosystem">Ecosystem</a></li><li><a class="dropdown__link" href="/autogen/docs/contributor-guide/contributing">Contributor Guide</a></li><li><a class="dropdown__link" href="/autogen/docs/Research">Research</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Examples</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/autogen/docs/Examples">Examples by Category</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/autogen/docs/notebooks">Examples by Notebook</a></li><li><a class="dropdown__link" href="/autogen/docs/Gallery">Application Gallery</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Other Languages</a><ul class="dropdown__menu"><li><a href="https://microsoft.github.io/autogen-for-net/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Dotnet<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><a class="navbar__item navbar__link" href="/autogen/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/microsoft/autogen" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://aka.ms/autogen-dc" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://twitter.com/pyautogen" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_Pkmr"><kbd class="searchHint_iIMx">ctrl</kbd><kbd class="searchHint_iIMx">K</kbd></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/autogen/docs/notebooks">Notebooks</a><button aria-label="Collapse sidebar category &#x27;Notebooks&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/JSON_mode_example">Mitigating Prompt hacking with JSON Mode in Autogen</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_RetrieveChat">Using RetrieveChat for Retrieve Augmented Code Generation and Question Answering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_RetrieveChat_mongodb">Using RetrieveChat Powered by MongoDB Atlas for Retrieve Augmented Code Generation and Question Answering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_RetrieveChat_pgvector">Using RetrieveChat Powered by PGVector for Retrieve Augmented Code Generation and Question Answering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_RetrieveChat_qdrant">Using RetrieveChat with Qdrant for Retrieve Augmented Code Generation and Question Answering</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_agentops">Agent Tracking with AgentOps</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_agentoptimizer">AgentOptimizer: An Agentic Way to Train Your LLM Agent</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_auto_feedback_from_code_execution">Task Solving with Code Generation, Execution and Debugging</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_azr_ai_search">Assistants with Azure Cognitive Search and Azure Identity</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_custom_model">Agent Chat with custom model loading</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_databricks_dbrx">Use AutoGen in Databricks with DBRX</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_function_call_async">Task Solving with Provided Tools as Functions (Asynchronous Function Calls)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_function_call_code_writing">Writing a software application using function calls</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_function_call_currency_calculator">Currency Calculator: Task Solving with Provided Tools as Functions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_group_chat_with_llamaindex_agents">Groupchat with Llamaindex agents</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_groupchat">Group Chat</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_groupchat_RAG">Group Chat with Retrieval Augmented Generation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_groupchat_customized">Group Chat with Customized Speaker Selection Method</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_groupchat_finite_state_machine">FSM - User can input speaker transition constraints</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_groupchat_research">Perform Research with Multi-Agent Group Chat</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/autogen/docs/notebooks/agentchat_groupchat_stateflow">StateFlow: Build Workflows through State-Oriented Actions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_groupchat_vis">Group Chat with Coder and Visualization Critic</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_image_generation_capability">Generate Dalle Images With Conversable Agents</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_lmm_gpt-4v">Engaging with Multimodal Models: GPT-4V in AutoGen</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_logging">Runtime Logging with AutoGen</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_multi_task_async_chats">Solving Multiple Tasks in a Sequence of Async Chats</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_multi_task_chats">Solving Multiple Tasks in a Sequence of Chats</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_nested_chats_chess">Nested Chats for Tool Use in Conversational Chess</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_nested_chats_chess_altmodels">Conversational Chess using non-OpenAI clients</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_nested_sequential_chats">Solving Complex Tasks with A Sequence of Nested Chats</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_nestedchat">Solving Complex Tasks with Nested Chats</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_nestedchat_optiguide">OptiGuide with Nested Chats in AutoGen</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_oai_assistant_function_call">Chat with OpenAI Assistant using function call in AutoGen: OSS Insights for Advanced GitHub Data Analysis</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_oai_assistant_groupchat">Auto Generated Agent Chat: Group Chat with GPTAssistantAgent</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_oai_code_interpreter">Auto Generated Agent Chat: GPTAssistant with Code Interpreter</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_society_of_mind">SocietyOfMindAgent</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_teachability">Chatting with a teachable agent</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_teaching">Auto Generated Agent Chat: Teaching</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_transform_messages">Preprocessing Chat History with `TransformMessages`</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_video_transcript_translate_with_whisper">Translating Video audio using Whisper and GPT-3.5-turbo</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_webscraping_with_apify">Web Scraping using Apify Tools</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchat_websockets">Websockets: Streaming input and output using websockets</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/agentchats_sequential_chats">Solving Multiple Tasks in a Sequence of Chats with Different Conversable Agent Pairs</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/notebooks/gpt_assistant_agent_function_call">From Dad Jokes To Sad Jokes: Function Calling with GPTAssistantAgent</a></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/autogen/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/autogen/docs/notebooks"><span itemprop="name">Notebooks</span></a><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">StateFlow: Build Workflows through State-Oriented Actions</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>StateFlow: Build Workflows through State-Oriented Actions</h1>
<p><a href="https://colab.research.google.com/github/microsoft/autogen/blob/main/notebook/agentchat_groupchat_stateflow.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" class="img_ev3q"></a>
<a href="https://github.com/microsoft/autogen/blob/main/notebook/agentchat_groupchat_stateflow.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github" alt="Open on GitHub" class="img_ev3q"></a></p>
<!-- -->
<p>AutoGen offers conversable agents powered by LLM, tool or human, which
can be used to perform tasks collectively via automated chat. In this
notebook, we introduce how to use groupchat to build workflows with
AutoGen agents from a state-oriented perspective.</p>
<div class="theme-admonition theme-admonition-info admonition_xJq3 alert alert--info"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 14 16"><path fill-rule="evenodd" d="M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z"></path></svg></span>Requirements</div><div class="admonitionContent_BuS1"><p>Install <code>pyautogen</code>:</p><div class="language-bash codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-bash codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">pip install pyautogen</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>For more information, please refer to the <a href="/autogen/docs/installation/">installation guide</a>.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="set-your-api-endpoint">Set your API Endpoint<a href="#set-your-api-endpoint" class="hash-link" aria-label="Direct link to Set your API Endpoint" title="Direct link to Set your API Endpoint">​</a></h2>
<p>The
<a href="https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json" target="_blank" rel="noopener noreferrer"><code>config_list_from_json</code></a>
function loads a list of configurations from an environment variable or
a json file.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> autogen</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">config_list </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">config_list_from_json</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;OAI_CONFIG_LIST&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    filter_dict</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;tags&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;gpt-4&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;gpt-4-32k&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="theme-admonition theme-admonition-tip admonition_xJq3 alert alert--success"><div class="admonitionHeading_Gvgb"><span class="admonitionIcon_Rf37"><svg viewBox="0 0 12 16"><path fill-rule="evenodd" d="M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"></path></svg></span>tip</div><div class="admonitionContent_BuS1"><p>Learn more about configuring LLMs for agents <a href="/autogen/docs/topics/llm_configuration">here</a>.</p></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="a-workflow-for-research">A workflow for research<a href="#a-workflow-for-research" class="hash-link" aria-label="Direct link to A workflow for research" title="Direct link to A workflow for research">​</a></h2>
<div><figure></figure></div>
<img src="../website/blog/2024-02-29-StateFlow/img/sf_example_1.png" width="700" alt="SF_Example_1">
<div></div>
<p>We define the following agents: - Initializer: Start the workflow by
sending a task. - Coder: Retrieve papers from the internet by writing
code. - Executor: Execute the code. - Scientist: Read the papers and
write a summary.</p>
<p>In the Figure, we define a simple workflow for research with 4 states:
Init, Retrieve, Research and End. Within each state, we will call
different agents to perform the tasks. - Init: We use the initializer to
start the workflow. - Retrieve: We will first call the coder to write
code and then call the executor to execute the code. - Research: We will
call the scientist to read the papers and write a summary. - End: We
will end the workflow.</p>
<p>Through customizing the speaker selection method, we can easily realize
the state-oriented workflow by defining the transitions between
different agents.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> tempfile</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">from</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">coding </span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> LocalCommandLineCodeExecutor</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">temp_dir </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> tempfile</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">TemporaryDirectory</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">executor </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> LocalCommandLineCodeExecutor</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    timeout</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">10</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">  </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># Timeout for each code execution in seconds.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    work_dir</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">temp_dir</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">name</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">  </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># Use the temporary directory to store the code files.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">gpt4_config </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;cache_seed&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token boolean" style="color:rgb(255, 88, 116)">False</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">  </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># change the cache_seed for different trials</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;temperature&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">0</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;config_list&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> config_list</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;timeout&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">120</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">initializer </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">UserProxyAgent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    name</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;Init&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    code_execution_config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token boolean" style="color:rgb(255, 88, 116)">False</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">coder </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">AssistantAgent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    name</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;Retrieve_Action_1&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    llm_config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">gpt4_config</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    system_message</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&quot;&quot;&quot;You are the Coder. Given a topic, write code to retrieve related papers from the arXiv API, print their title, authors, abstract, and link.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">You write python/shell code to solve tasks. Wrap the code in a code block that specifies the script type. The user can&#x27;t modify your code. So do not suggest incomplete code which requires others to modify. Don&#x27;t use a code block if it&#x27;s not intended to be executed by the executor.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">Don&#x27;t include multiple code blocks in one response. Do not ask others to copy and paste the result. Check the execution result returned by the executor.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can&#x27;t be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&quot;&quot;&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">executor </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">UserProxyAgent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    name</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;Retrieve_Action_2&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    system_message</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;Executor. Execute the code written by the Coder and report the result.&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    human_input_mode</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;NEVER&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    code_execution_config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;executor&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> executor</span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">scientist </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">AssistantAgent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    name</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;Research_Action_1&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    llm_config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">gpt4_config</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    system_message</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&quot;&quot;&quot;You are the Scientist. Please categorize papers after seeing their abstracts printed and create a markdown table with Domain, Title, Authors, Summary and Link&quot;&quot;&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(130, 170, 255)">state_transition</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">last_speaker</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> groupchat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    messages </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> groupchat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">messages</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token keyword" style="font-style:italic">if</span><span class="token plain"> last_speaker </span><span class="token keyword" style="font-style:italic">is</span><span class="token plain"> initializer</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># init -&gt; retrieve</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token keyword" style="font-style:italic">return</span><span class="token plain"> coder</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token keyword" style="font-style:italic">elif</span><span class="token plain"> last_speaker </span><span class="token keyword" style="font-style:italic">is</span><span class="token plain"> coder</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># retrieve: action 1 -&gt; action 2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token keyword" style="font-style:italic">return</span><span class="token plain"> executor</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token keyword" style="font-style:italic">elif</span><span class="token plain"> last_speaker </span><span class="token keyword" style="font-style:italic">is</span><span class="token plain"> executor</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token keyword" style="font-style:italic">if</span><span class="token plain"> messages</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token operator" style="color:rgb(137, 221, 255)">-</span><span class="token number" style="color:rgb(247, 140, 108)">1</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;content&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">==</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;exitcode: 1&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># retrieve --(execution failed)--&gt; retrieve</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token keyword" style="font-style:italic">return</span><span class="token plain"> coder</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token keyword" style="font-style:italic">else</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># retrieve --(execution sucess)--&gt; research</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token keyword" style="font-style:italic">return</span><span class="token plain"> scientist</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token keyword" style="font-style:italic">elif</span><span class="token plain"> last_speaker </span><span class="token operator" style="color:rgb(137, 221, 255)">==</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;Scientist&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># research -&gt; end</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token keyword" style="font-style:italic">return</span><span class="token plain"> </span><span class="token boolean" style="color:rgb(255, 88, 116)">None</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">groupchat </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">GroupChat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    agents</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token plain">initializer</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> coder</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> executor</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> scientist</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    messages</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    max_round</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">20</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    speaker_selection_method</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">state_transition</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">manager </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">GroupChatManager</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">groupchat</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">groupchat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> llm_config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">gpt4_config</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">chat_result </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> initializer</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">initiate_chat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    manager</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> message</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;Topic: LLM applications papers from last week. Requirement: 5 - 10 papers from different domains.&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token message-id class-name" style="color:rgb(255, 203, 107)">Init</span><span class="token message-id"> </span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token message-id keyword" style="font-style:italic">to</span><span class="token message-id"> </span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">chat_manager</span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token message-id">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Topic: LLM applications papers from last week. Requirement: 5 - 10 papers from different domains.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">--------------------------------------------------------------------------------</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">Retrieve_Action_1</span><span class="token message-id"> </span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token message-id keyword" style="font-style:italic">to</span><span class="token message-id"> </span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">chat_manager</span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token message-id">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">To retrieve related papers from the arXiv API, we can use Python with the `requests` library to send a query to the API and parse the response. Below is a Python script that searches for papers related to &quot;LLM applications&quot; (Large Language Models applications) from the last week, across different domains, and prints out the required information for 5 to 10 papers.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```python</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">import requests</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">from datetime import datetime, timedelta</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">import feedparser</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># Define the base URL for the arXiv API</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">ARXIV_API_URL = &quot;http://export.arxiv.org/api/query?&quot;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># Define the search parameters</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">search_query = &quot;all:\&quot;LLM applications\&quot;&quot;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">start = 0</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">max_results = 10</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">sort_by = &quot;submittedDate&quot;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">sort_order = &quot;descending&quot;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># Calculate the date one week ago from today</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">one_week_ago = (datetime.now() - timedelta(days=7)).strftime(&#x27;%Y-%m-%dT%H:%M:%SZ&#x27;)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># Construct the query</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">query = f&quot;search_query={search_query}&amp;start={start}&amp;max_results={max_results}&amp;sortBy={sort_by}&amp;sortOrder={sort_order}&amp;submittedDateRange={one_week_ago}-&quot;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># Send the request to the arXiv API</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">response = requests.get(ARXIV_API_URL + query)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># Parse the response using feedparser</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">feed = feedparser.parse(response.content)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># Print the title, authors, abstract, and link of each paper</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">for entry in feed.entries:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    print(&quot;Title:&quot;, entry.title)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    print(&quot;Authors:&quot;, &#x27;, &#x27;.join(author.name for author in entry.authors))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    print(&quot;Abstract:&quot;, entry.summary)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    print(&quot;Link:&quot;, entry.link)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    print(&quot;\n---\n&quot;)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">This script will print the title, authors, abstract, and link for each paper related to &quot;LLM applications&quot; that was submitted in the last week, up to a maximum of 10 papers. If you want to ensure that the papers are from different domains, you might need to manually check the categories of the papers or refine the search query to target specific domains.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">--------------------------------------------------------------------------------</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token function" style="color:rgb(130, 170, 255)">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; EXECUTING CODE BLOCK (inferred language is python)...</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token function" style="color:rgb(130, 170, 255)"></span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">Retrieve_Action_2</span><span class="token message-id"> </span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token message-id keyword" style="font-style:italic">to</span><span class="token message-id"> </span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">chat_manager</span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token message-id">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">exitcode: 0 (execution succeeded)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Code output: Title: Adapting LLMs for Efficient Context Processing through Soft Prompt</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  Compression</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Authors: Cangqing Wang, Yutian Yang, Ruisi Li, Dan Sun, Ruicong Cai, Yuzhu Zhang, Chengqian Fu, Lillian Floyd</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Abstract: The rapid advancement of Large Language Models (LLMs) has inaugurated a</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">transformative epoch in natural language processing, fostering unprecedented</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">proficiency in text generation, comprehension, and contextual scrutiny.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Nevertheless, effectively handling extensive contexts, crucial for myriad</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">applications, poses a formidable obstacle owing to the intrinsic constraints of</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">the models&#x27; context window sizes and the computational burdens entailed by</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">their operations. This investigation presents an innovative framework that</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">strategically tailors LLMs for streamlined context processing by harnessing the</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">synergies among natural language summarization, soft prompt compression, and</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">augmented utility preservation mechanisms. Our methodology, dubbed</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">SoftPromptComp, amalgamates natural language prompts extracted from</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">summarization methodologies with dynamically generated soft prompts to forge a</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">concise yet semantically robust depiction of protracted contexts. This</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">depiction undergoes further refinement via a weighting mechanism optimizing</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">information retention and utility for subsequent tasks. We substantiate that</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">our framework markedly diminishes computational overhead and enhances LLMs&#x27;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">efficacy across various benchmarks, while upholding or even augmenting the</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">caliber of the produced content. By amalgamating soft prompt compression with</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">sophisticated summarization, SoftPromptComp confronts the dual challenges of</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">managing lengthy contexts and ensuring model scalability. Our findings point</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">towards a propitious trajectory for augmenting LLMs&#x27; applicability and</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">efficiency, rendering them more versatile and pragmatic for real-world</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">applications. This research enriches the ongoing discourse on optimizing</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">language models, providing insights into the potency of soft prompts and</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">summarization techniques as pivotal instruments for the forthcoming generation</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">of NLP solutions.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Link: http://arxiv.org/abs/2404.04997v1</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">---</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Title: Explainable Traffic Flow Prediction with Large Language Models</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Authors: Xusen Guo, Qiming Zhang, Mingxing Peng, Meixin Zhu, Hao, Yang</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Abstract: Traffic flow prediction is crucial for urban planning, transportation</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">management, and infrastructure development. However, achieving both accuracy</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">and interpretability in prediction models remains challenging due to the</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">complexity of traffic data and the inherent opacity of deep learning</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">methodologies. In this paper, we propose a novel approach, Traffic Flow</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Prediction LLM (TF-LLM), which leverages large language models (LLMs) to</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">generate interpretable traffic flow predictions. By transferring multi-modal</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">traffic data into natural language descriptions, TF-LLM captures complex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">spatial-temporal patterns and external factors such as weather conditions,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Points of Interest (PoIs), date, and holidays. We fine-tune the LLM framework</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">using language-based instructions to align with spatial-temporal traffic flow</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">data. Our comprehensive multi-modal traffic flow dataset (CATraffic) in</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">California enables the evaluation of TF-LLM against state-of-the-art deep</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">learning baselines. Results demonstrate TF-LLM&#x27;s competitive accuracy while</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">providing intuitive and interpretable predictions. We discuss the</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">spatial-temporal and input dependencies for explainable future flow</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">forecasting, showcasing TF-LLM&#x27;s potential for diverse city prediction tasks.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">This paper contributes to advancing explainable traffic prediction models and</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">lays a foundation for future exploration of LLM applications in transportation.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Link: http://arxiv.org/abs/2404.02937v2</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">---</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Title: Designing Child-Centric AI Learning Environments: Insights from</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  LLM-Enhanced Creative Project-Based Learning</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Authors: Siyu Zha, Yuehan Qiao, Qingyu Hu, Zhongsheng Li, Jiangtao Gong, Yingqing Xu</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Abstract: Project-based learning (PBL) is an instructional method that is very helpful</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">in nurturing students&#x27; creativity, but it requires significant time and energy</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">from both students and teachers. Large language models (LLMs) have been proven</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">to assist in creative tasks, yet much controversy exists regarding their role</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">in fostering creativity. This paper explores the potential of LLMs in PBL</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">settings, with a special focus on fostering creativity. We began with an</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">exploratory study involving 12 middle school students and identified five</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">design considerations for LLM applications in PBL. Building on this, we</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">developed an LLM-empowered, 48-hour PBL program and conducted an instructional</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">experiment with 31 middle school students. Our results indicated that LLMs can</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">enhance every stage of PBL. Additionally, we also discovered ambivalent</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">perspectives among students and mentors toward LLM usage. Furthermore, we</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">explored the challenge and design implications of integrating LLMs into PBL and</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">reflected on the program. By bridging AI advancements into educational</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">practice, our work aims to inspire further discourse and investigation into</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">harnessing AI&#x27;s potential in child-centric educational settings.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Link: http://arxiv.org/abs/2403.16159v2</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">---</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Title: The opportunities and risks of large language models in mental health</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Authors: Hannah R. Lawrence, Renee A. Schneider, Susan B. Rubin, Maja J. Mataric, Daniel J. McDuff, Megan Jones Bell</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Abstract: Global rates of mental health concerns are rising and there is increasing</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">realization that existing models of mental healthcare will not adequately</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">expand to meet the demand. With the emergence of large language models (LLMs)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">has come great optimism regarding their promise to create novel, large-scale</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">solutions to support mental health. Despite their nascence, LLMs have already</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">been applied to mental health-related tasks. In this review, we summarize the</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">extant literature on efforts to use LLMs to provide mental health education,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">assessment, and intervention and highlight key opportunities for positive</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">impact in each area. We then highlight risks associated with LLMs application</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">to mental health and encourage adoption of strategies to mitigate these risks.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">The urgent need for mental health support must be balanced with responsible</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">development, testing, and deployment of mental health LLMs. Especially critical</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">is ensuring that mental health LLMs are fine-tuned for mental health, enhance</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">mental health equity, adhere to ethical standards, and that people, including</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">those with lived experience with mental health concerns, are involved in all</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">stages from development through deployment. Prioritizing these efforts will</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">minimize potential harms to mental health and maximize the likelihood that LLMs</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">will positively impact mental health globally.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Link: http://arxiv.org/abs/2403.14814v2</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">---</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Title: Large Language Models for Blockchain Security: A Systematic Literature</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  Review</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Authors: Zheyuan He, Zihao Li, Sen Yang</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Abstract: Large Language Models (LLMs) have emerged as powerful tools in various</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">domains involving blockchain security (BS). Several recent studies are</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">exploring LLMs applied to BS. However, there remains a gap in our understanding</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">regarding the full scope of applications, impacts, and potential constraints of</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">LLMs on blockchain security. To fill this gap, we conduct a literature review</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">on LLM4BS.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  As the first review of LLM&#x27;s application on blockchain security, our study</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">aims to comprehensively analyze existing research and elucidate how LLMs</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">contribute to enhancing the security of blockchain systems. Through a thorough</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">examination of scholarly works, we delve into the integration of LLMs into</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">various aspects of blockchain security. We explore the mechanisms through which</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">LLMs can bolster blockchain security, including their applications in smart</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">contract auditing, identity verification, anomaly detection, vulnerable repair,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">and so on. Furthermore, we critically assess the challenges and limitations</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">associated with leveraging LLMs for blockchain security, considering factors</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">such as scalability, privacy concerns, and adversarial attacks. Our review</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">sheds light on the opportunities and potential risks inherent in this</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">convergence, providing valuable insights for researchers, practitioners, and</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">policymakers alike.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Link: http://arxiv.org/abs/2403.14280v2</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">---</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Title: Do Large Language Model Understand Multi-Intent Spoken Language ?</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Authors: Shangjian Yin, Peijie Huang, Yuhong Xu, Haojing Huang, Jiatian Chen</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Abstract: This study marks a significant advancement by harnessing Large Language</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Models (LLMs) for multi-intent spoken language understanding (SLU), proposing a</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">unique methodology that capitalizes on the generative power of LLMs within an</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">SLU context. Our innovative technique reconfigures entity slots specifically</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">for LLM application in multi-intent SLU environments and introduces the concept</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">of Sub-Intent Instruction (SII), enhancing the dissection and interpretation of</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">intricate, multi-intent communication within varied domains. The resultant</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">datasets, dubbed LM-MixATIS and LM-MixSNIPS, are crafted from pre-existing</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">benchmarks. Our research illustrates that LLMs can match and potentially excel</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">beyond the capabilities of current state-of-the-art multi-intent SLU models. It</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">further explores LLM efficacy across various intent configurations and dataset</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">proportions. Moreover, we introduce two pioneering metrics, Entity Slot</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Accuracy (ESA) and Combined Semantic Accuracy (CSA), to provide an in-depth</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">analysis of LLM proficiency in this complex field.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Link: http://arxiv.org/abs/2403.04481v2</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">---</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Title: Breaking the Language Barrier: Can Direct Inference Outperform</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  Pre-Translation in Multilingual LLM Applications?</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Authors: Yotam Intrator, Matan Halfon, Roman Goldenberg, Reut Tsarfaty, Matan Eyal, Ehud Rivlin, Yossi Matias, Natalia Aizenberg</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Abstract: Large language models hold significant promise in multilingual applications.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">However, inherent biases stemming from predominantly English-centric</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">pre-training have led to the widespread practice of pre-translation, i.e.,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">translating non-English inputs to English before inference, leading to</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">complexity and information loss. This study re-evaluates the need for</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">pre-translation in the context of PaLM2 models (Anil et al., 2023), which have</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">been established as highly performant in multilingual tasks. We offer a</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">comprehensive investigation across 108 languages and 6 diverse benchmarks,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">including open-end generative tasks, which were excluded from previous similar</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">studies. Our findings challenge the pre-translation paradigm established in</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">prior research, highlighting the advantages of direct inference in PaLM2.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Specifically, PaLM2-L consistently outperforms pre-translation in 94 out of 108</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">languages. These findings pave the way for more efficient and effective</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">multilingual applications, alleviating the limitations associated with</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">pre-translation and unlocking linguistic authenticity.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Link: http://arxiv.org/abs/2403.04792v1</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">---</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Title: SciAssess: Benchmarking LLM Proficiency in Scientific Literature</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  Analysis</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Authors: Hengxing Cai, Xiaochen Cai, Junhan Chang, Sihang Li, Lin Yao, Changxin Wang, Zhifeng Gao, Hongshuai Wang, Yongge Li, Mujie Lin, Shuwen Yang, Jiankun Wang, Yuqi Yin, Yaqi Li, Linfeng Zhang, Guolin Ke</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Abstract: Recent breakthroughs in Large Language Models (LLMs) have revolutionized</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">natural language understanding and generation, igniting a surge of interest in</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">leveraging these technologies in the field of scientific literature analysis.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Existing benchmarks, however, inadequately evaluate the proficiency of LLMs in</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">scientific literature analysis, especially in scenarios involving complex</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">comprehension and multimodal data. In response, we introduced SciAssess, a</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">benchmark tailored for the in-depth analysis of scientific literature, crafted</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">to provide a thorough assessment of LLMs&#x27; efficacy. SciAssess focuses on</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">evaluating LLMs&#x27; abilities in memorization, comprehension, and analysis within</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">the context of scientific literature analysis. It includes representative tasks</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">from diverse scientific fields, such as general chemistry, organic materials,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">and alloy materials. And rigorous quality control measures ensure its</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">reliability in terms of correctness, anonymization, and copyright compliance.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">SciAssess evaluates leading LLMs, including GPT-4, GPT-3.5, and Gemini,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">identifying their strengths and aspects for improvement and supporting the</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">ongoing development of LLM applications in scientific literature analysis.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">SciAssess and its resources are made available at https://sci-assess.github.io,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">offering a valuable tool for advancing LLM capabilities in scientific</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">literature analysis.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Link: http://arxiv.org/abs/2403.01976v2</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">---</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Title: Differentially Private Synthetic Data via Foundation Model APIs 2: Text</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Authors: Chulin Xie, Zinan Lin, Arturs Backurs, Sivakanth Gopi, Da Yu, Huseyin A Inan, Harsha Nori, Haotian Jiang, Huishuai Zhang, Yin Tat Lee, Bo Li, Sergey Yekhanin</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Abstract: Text data has become extremely valuable due to the emergence of machine</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">learning algorithms that learn from it. A lot of high-quality text data</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">generated in the real world is private and therefore cannot be shared or used</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">freely due to privacy concerns. Generating synthetic replicas of private text</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">data with a formal privacy guarantee, i.e., differential privacy (DP), offers a</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">promising and scalable solution. However, existing methods necessitate DP</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">finetuning of large language models (LLMs) on private data to generate DP</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">synthetic data. This approach is not viable for proprietary LLMs (e.g.,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">GPT-3.5) and also demands considerable computational resources for open-source</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">LLMs. Lin et al. (2024) recently introduced the Private Evolution (PE)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">algorithm to generate DP synthetic images with only API access to diffusion</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">models. In this work, we propose an augmented PE algorithm, named Aug-PE, that</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">applies to the complex setting of text. We use API access to an LLM and</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">generate DP synthetic text without any model training. We conduct comprehensive</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">experiments on three benchmark datasets. Our results demonstrate that Aug-PE</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">produces DP synthetic text that yields competitive utility with the SOTA DP</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">finetuning baselines. This underscores the feasibility of relying solely on API</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">access of LLMs to produce high-quality DP synthetic texts, thereby facilitating</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">more accessible routes to privacy-preserving LLM applications. Our code and</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">data are available at https://github.com/AI-secure/aug-pe.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Link: http://arxiv.org/abs/2403.01749v1</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">---</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Title: SERVAL: Synergy Learning between Vertical Models and LLMs towards</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  Oracle-Level Zero-shot Medical Prediction</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Authors: Jiahuan Yan, Jintai Chen, Chaowen Hu, Bo Zheng, Yaojun Hu, Jimeng Sun, Jian Wu</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Abstract: Recent development of large language models (LLMs) has exhibited impressive</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">zero-shot proficiency on generic and common sense questions. However, LLMs&#x27;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">application on domain-specific vertical questions still lags behind, primarily</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">due to the humiliation problems and deficiencies in vertical knowledge.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Furthermore, the vertical data annotation process often requires</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">labor-intensive expert involvement, thereby presenting an additional challenge</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">in enhancing the model&#x27;s vertical capabilities. In this paper, we propose</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">SERVAL, a synergy learning pipeline designed for unsupervised development of</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">vertical capabilities in both LLMs and small models by mutual enhancement.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Specifically, SERVAL utilizes the LLM&#x27;s zero-shot outputs as annotations,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">leveraging its confidence to teach a robust vertical model from scratch.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Reversely, the trained vertical model guides the LLM fine-tuning to enhance its</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">zero-shot capability, progressively improving both models through an iterative</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">process. In medical domain, known for complex vertical knowledge and costly</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">annotations, comprehensive experiments show that, without access to any gold</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">labels, SERVAL with the synergy learning of OpenAI GPT-3.5 and a simple model</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">attains fully-supervised competitive performance across ten widely used medical</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">datasets. These datasets represent vertically specialized medical diagnostic</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">scenarios (e.g., diabetes, heart diseases, COVID-19), highlighting the</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">potential of SERVAL in refining the vertical capabilities of LLMs and training</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">vertical models from scratch, all achieved without the need for annotations.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Link: http://arxiv.org/abs/2403.01570v2</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">---</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">--------------------------------------------------------------------------------</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">Research_Action_1</span><span class="token message-id"> </span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token message-id keyword" style="font-style:italic">to</span><span class="token message-id"> </span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">chat_manager</span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token message-id">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Based on the provided code output, here is a markdown table categorizing the papers by domain, along with their titles, authors, summaries, and links:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Domain | Title | Authors | Summary | Link |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">|</span><span class="token punctuation" style="color:rgb(199, 146, 234)">--------</span><span class="token plain">|</span><span class="token punctuation" style="color:rgb(199, 146, 234)">-------</span><span class="token plain">|</span><span class="token punctuation" style="color:rgb(199, 146, 234)">---------</span><span class="token plain">|</span><span class="token punctuation" style="color:rgb(199, 146, 234)">---------</span><span class="token plain">|</span><span class="token punctuation" style="color:rgb(199, 146, 234)">------</span><span class="token plain">|</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Natural Language Processing | Adapting LLMs for Efficient Context Processing through Soft Prompt Compression | Cangqing Wang, et al. | The paper presents a framework for efficient context processing in LLMs using natural language summarization and soft prompt compression. | [Link](http://arxiv.org/abs/2404.04997v1) |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Transportation | Explainable Traffic Flow Prediction with Large Language Models | Xusen Guo, et al. | This paper introduces a novel approach for interpretable traffic flow predictions using LLMs, which captures complex spatial-temporal patterns. | [Link](http://arxiv.org/abs/2404.02937v2) |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Education | Designing Child-Centric AI Learning Environments: Insights from LLM-Enhanced Creative Project-Based Learning | Siyu Zha, et al. | The study explores the potential of LLMs in enhancing project-based learning (PBL) and fostering creativity in educational settings. | [Link](http://arxiv.org/abs/2403.16159v2) |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Mental Health | The opportunities and risks of large language models in mental health | Hannah R. Lawrence, et al. | This review summarizes the literature on LLMs in mental health education, assessment, and intervention, highlighting opportunities and risks. | [Link](http://arxiv.org/abs/2403.14814v2) |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Blockchain Security | Large Language Models for Blockchain Security: A Systematic Literature Review | Zheyuan He, et al. | The paper reviews the application of LLMs in blockchain security, discussing their impact and potential limitations. | [Link](http://arxiv.org/abs/2403.14280v2) |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Spoken Language Understanding | Do Large Language Model Understand Multi-Intent Spoken Language? | Shangjian Yin, et al. | The study investigates LLMs&#x27; capabilities in multi-intent spoken language understanding and proposes new methodologies and metrics. | [Link](http://arxiv.org/abs/2403.04481v2) |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Multilingualism | Breaking the Language Barrier: Can Direct Inference Outperform Pre-Translation in Multilingual LLM Applications? | Yotam Intrator, et al. | The paper challenges the pre-translation paradigm in multilingual LLM applications, showing the advantages of direct inference. | [Link](http://arxiv.org/abs/2403.04792v1) |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Scientific Literature | SciAssess: Benchmarking LLM Proficiency in Scientific Literature Analysis | Hengxing Cai, et al. | Introduces SciAssess, a benchmark for evaluating LLMs&#x27; abilities in scientific literature analysis across various scientific fields. | [Link](http://arxiv.org/abs/2403.01976v2) |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Privacy &amp; Security | Differentially Private Synthetic Data via Foundation Model APIs 2: Text | Chulin Xie, et al. | The paper proposes a method to generate differentially private synthetic text data using API access to LLMs without model training. | [Link](http://arxiv.org/abs/2403.01749v1) |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Medical Diagnostics | SERVAL: Synergy Learning between Vertical Models and LLMs towards Oracle-Level Zero-shot Medical Prediction | Jiahuan Yan, et al. | SERVAL is a synergy learning pipeline that enhances the vertical capabilities of LLMs and trains vertical models without annotations in the medical domain. | [Link](http://arxiv.org/abs/2403.01570v2) |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Please note that the domains have been inferred from the summaries and titles of the papers and may not perfectly reflect the authors&#x27; intended categorization.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">--------------------------------------------------------------------------------</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/autogen/docs/tags/orchestration">orchestration</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/autogen/docs/tags/group-chat">group chat</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/autogen/docs/tags/research">research</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/microsoft/autogen/edit/main/notebook/agentchat_groupchat_stateflow.ipynb" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/autogen/docs/notebooks/agentchat_groupchat_research"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Perform Research with Multi-Agent Group Chat</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/autogen/docs/notebooks/agentchat_groupchat_vis"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Group Chat with Coder and Visualization Critic</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#set-your-api-endpoint" class="table-of-contents__link toc-highlight">Set your API Endpoint</a></li><li><a href="#a-workflow-for-research" class="table-of-contents__link toc-highlight">A workflow for research</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://aka.ms/autogen-dc" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/pyautogen" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 AutoGen Authors |  <a target="_blank" style="color:#10adff" href="https://go.microsoft.com/fwlink/?LinkId=521839">Privacy and Cookies</a></div></div></div></footer></div>
</body>
</html>