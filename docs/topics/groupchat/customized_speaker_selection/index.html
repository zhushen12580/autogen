<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-topics/groupchat/customized_speaker_selection" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.1">
<title data-rh="true">Customize Speaker Selection | AutoGen</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://microsoft.github.io/autogen/docs/topics/groupchat/customized_speaker_selection"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Customize Speaker Selection | AutoGen"><meta data-rh="true" name="description" content="Custom Speaker Selection Function"><meta data-rh="true" property="og:description" content="Custom Speaker Selection Function"><link data-rh="true" rel="icon" href="/autogen/img/ag.ico"><link data-rh="true" rel="canonical" href="https://microsoft.github.io/autogen/docs/topics/groupchat/customized_speaker_selection"><link data-rh="true" rel="alternate" href="https://microsoft.github.io/autogen/docs/topics/groupchat/customized_speaker_selection" hreflang="en"><link data-rh="true" rel="alternate" href="https://microsoft.github.io/autogen/docs/topics/groupchat/customized_speaker_selection" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/autogen/blog/rss.xml" title="AutoGen RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/autogen/blog/atom.xml" title="AutoGen Atom Feed">






<script>!function(e,t,n,c,s,a,r){e[n]=e[n]||function(){(e[n].q=e[n].q||[]).push(arguments)},(a=t.createElement(c)).async=1,a.src="https://www.clarity.ms/tag/lnxpe6skj1",(r=t.getElementsByTagName(c)[0]).parentNode.insertBefore(a,r)}(window,document,"clarity","script")</script>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.13.11/dist/katex.min.css" integrity="sha384-Um5gpz1odJg5Z4HAmzPtgZKdTBHZdw8S29IecapCSB31ligYPhHQZMIlWLYQGVoc" crossorigin="anonymous">
<script src="/autogen/js/custom.js" async defer="defer"></script><link rel="stylesheet" href="/autogen/assets/css/styles.d8ef5d37.css">
<script src="/autogen/assets/js/runtime~main.33d2ca05.js" defer="defer"></script>
<script src="/autogen/assets/js/main.f5c79556.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const a=new URLSearchParams(window.location.search).entries();for(var[t,e]of a)if(t.startsWith("docusaurus-data-")){var n=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(n,e)}}catch(t){}}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="announcementBar_mb4j" style="background-color:#fafbfc;color:#091E42" role="banner"><div class="announcementBarPlaceholder_vyr4"></div><div class="content_knG7 announcementBarContent_xLdY">What's new in AutoGen? Read <a href="/autogen/blog/2024/03/03/AutoGen-Update">this blog</a> for an overview of updates</div><button type="button" aria-label="Close" class="clean-btn close closeButton_CVFx announcementBarClose_gvF7"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/autogen/"><div class="navbar__logo"><img src="/autogen/img/ag.svg" alt="AutoGen" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/autogen/img/ag.svg" alt="AutoGen" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">AutoGen</b></a><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Docs</a><ul class="dropdown__menu"><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/autogen/docs/Getting-Started">Getting Started</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/autogen/docs/installation/">Installation</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/autogen/docs/tutorial/introduction">Tutorial</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/autogen/docs/topics">User Guide</a></li><li><a class="dropdown__link" href="/autogen/docs/reference/agentchat/conversable_agent">API Reference</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/autogen/docs/FAQ">FAQs</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/autogen/docs/autogen-studio/getting-started">AutoGen Studio</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/autogen/docs/ecosystem">Ecosystem</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/autogen/docs/contributor-guide/contributing">Contributor Guide</a></li><li><a aria-current="page" class="dropdown__link dropdown__link--active" href="/autogen/docs/Research">Research</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Examples</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/autogen/docs/Examples">Examples by Category</a></li><li><a class="dropdown__link" href="/autogen/docs/notebooks">Examples by Notebook</a></li><li><a class="dropdown__link" href="/autogen/docs/Gallery">Application Gallery</a></li></ul></div><div class="navbar__item dropdown dropdown--hoverable"><a href="#" aria-haspopup="true" aria-expanded="false" role="button" class="navbar__link">Other Languages</a><ul class="dropdown__menu"><li><a href="https://microsoft.github.io/autogen-for-net/" target="_blank" rel="noopener noreferrer" class="dropdown__link">Dotnet<svg width="12" height="12" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><a class="navbar__item navbar__link" href="/autogen/blog">Blog</a></div><div class="navbar__items navbar__items--right"><a href="https://github.com/microsoft/autogen" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://aka.ms/autogen-dc" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><a href="https://twitter.com/pyautogen" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"><div class="navbar__search searchBarContainer_NW3z"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing_RJI3 searchBarLoadingRing_YnHq"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_Pkmr"><kbd class="searchHint_iIMx">ctrl</kbd><kbd class="searchHint_iIMx">K</kbd></div></div></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG menuWithAnnouncementBar_GW3s"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/autogen/docs/Getting-Started">Getting Started</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/autogen/docs/installation/">Installation</a><button aria-label="Expand sidebar category &#x27;Installation&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="true" href="/autogen/docs/tutorial">Tutorial</a><button aria-label="Collapse sidebar category &#x27;Tutorial&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/tutorial/introduction">Introduction</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/tutorial/chat-termination">Chat Termination</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/tutorial/human-in-the-loop">Human in the Loop</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/tutorial/code-executors">Code Executors</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/tutorial/tool-use">Tool Use</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/tutorial/conversation-patterns">Conversation Patterns</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/tutorial/what-next">What Next?</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/autogen/docs/Use-Cases/agent_chat">Use Cases</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active" aria-expanded="true" href="/autogen/docs/topics">User Guide</a><button aria-label="Collapse sidebar category &#x27;User Guide&#x27;" type="button" class="clean-btn menu__caret"></button></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/autogen/docs/topics/code-execution/cli-code-executor">Code Execution</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/autogen/docs/topics/openai-assistant/gpt_assistant_agent">OpenAI Assistant</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" tabindex="0" href="/autogen/docs/topics/groupchat/customized_speaker_selection">GroupChat</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/autogen/docs/topics/groupchat/customized_speaker_selection">Customize Speaker Selection</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-3 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/topics/groupchat/resuming_groupchat">Resuming a GroupChat</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/autogen/docs/topics/non-openai-models/about-using-nonopenai-models">Using Non-OpenAI Models</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/autogen/docs/topics/handling_long_contexts/compressing_text_w_llmligua">Handling Long Contexts</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/topics/llm-caching">LLM Caching</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/topics/llm-observability">Agent Observability</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/topics/llm_configuration">LLM Configuration</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" tabindex="0" href="/autogen/docs/topics/prompting-and-reasoning/react">Prompting and Reasoning</a></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/topics/retrieval_augmentation">Retrieval Augmentation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/autogen/docs/topics/task_decomposition">Task Decomposition</a></li></ul></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/autogen/docs/reference/agentchat/conversable_agent">API Reference</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/autogen/docs/FAQ">FAQs</a></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/autogen/docs/autogen-studio">AutoGen Studio</a><button aria-label="Expand sidebar category &#x27;AutoGen Studio&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/autogen/docs/ecosystem">Ecosystem</a><button aria-label="Expand sidebar category &#x27;Ecosystem&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist" aria-expanded="false" href="/autogen/docs/contributor-guide">Contributor Guide</a><button aria-label="Expand sidebar category &#x27;Contributor Guide&#x27;" type="button" class="clean-btn menu__caret"></button></div></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/autogen/docs/Research">Research</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-1 menu__list-item"><a class="menu__link" href="/autogen/docs/Migration-Guide">Migration Guide</a></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/autogen/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item"><a class="breadcrumbs__link" itemprop="item" href="/autogen/docs/topics"><span itemprop="name">User Guide</span></a><meta itemprop="position" content="1"></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">GroupChat</span><meta itemprop="position" content="2"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Customize Speaker Selection</span><meta itemprop="position" content="3"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><h1>Customize Speaker Selection</h1>
<p><a href="https://colab.research.google.com/github/microsoft/autogen/blob/main/website/docs/topics/groupchat/customized_speaker_selection.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab" class="img_ev3q"></a>
<a href="https://github.com/microsoft/autogen/blob/main/website/docs/topics/groupchat/customized_speaker_selection.ipynb" target="_blank" rel="noopener noreferrer"><img decoding="async" loading="lazy" src="https://img.shields.io/badge/Open%20on%20GitHub-grey?logo=github" alt="Open on GitHub" class="img_ev3q"></a></p>
<p><img decoding="async" loading="lazy" alt="group_chat" src="/autogen/assets/images/sf_example_1-4e72bff416bab1de6a634bdcc552c199.png" width="1226" height="472" class="img_ev3q"></p>
<p>In GroupChat, we can customize the speaker selection by passing a
function to the <code>GroupChat</code> object. With this function, you can build a
more <strong>deterministic</strong> agent workflow. We recommend following a
<strong>StateFlow</strong> pattern when crafting this function. Please refer to the
<a href="/autogen/blog/2024/02/29/StateFlow">StateFlow blog</a> for more details.</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="an-example-research-workflow">An example research workflow<a href="#an-example-research-workflow" class="hash-link" aria-label="Direct link to An example research workflow" title="Direct link to An example research workflow">​</a></h2>
<p>We provide a simple example to build a StateFlow model for research with
customized speaker selection.</p>
<p>We first define the following agents:</p>
<ul>
<li>Initializer: Start the workflow by sending a task.</li>
<li>Coder: Retrieve papers from the internet by writing code.</li>
<li>Executor: Execute the code.</li>
<li>Scientist: Read the papers and write a summary.</li>
</ul>
<p>In the figure above, we define a simple workflow for research with 4
states: <em>Init</em>, <em>Retrieve</em>, <em>Research</em>, and <em>End</em>. Within each state, we
will call different agents to perform the tasks.</p>
<ul>
<li><em>Init</em>: We use the initializer to start the workflow.</li>
<li><em>Retrieve</em>: We will first call the coder to write code and then call
the executor to execute the code.</li>
<li><em>Research</em>: We will call the scientist to read the papers and write
a summary.</li>
<li><em>End</em>: We will end the workflow.</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="create-your-speaker-selection-function">Create your speaker selection function<a href="#create-your-speaker-selection-function" class="hash-link" aria-label="Direct link to Create your speaker selection function" title="Direct link to Create your speaker selection function">​</a></h2>
<p>Below is a skeleton of the speaker selection function. Fill in the
function to define the speaker selection logic.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(130, 170, 255)">custom_speaker_selection_func</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    last_speaker</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> Agent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    groupchat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> GroupChat</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">-</span><span class="token operator" style="color:rgb(137, 221, 255)">&gt;</span><span class="token plain"> Union</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token plain">Agent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> Literal</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;auto&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;manual&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;random&#x27;</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&#x27;round_robin&#x27;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> </span><span class="token boolean" style="color:rgb(255, 88, 116)">None</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&quot;&quot;&quot;Define a customized speaker selection function.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    A recommended way is to define a transition for each speaker in the groupchat.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="display:inline-block;color:rgb(195, 232, 141)"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    Parameters:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">        - last_speaker: Agent</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">            The last speaker in the group chat.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">        - groupchat: GroupChat</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">            The GroupChat object</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    Return:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">        Return one of the following:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">        1. an `Agent` class, it must be one of the agents in the group chat.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">        2. a string from [&#x27;auto&#x27;, &#x27;manual&#x27;, &#x27;random&#x27;, &#x27;round_robin&#x27;] to select a default method to use.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">        3. None, which indicates the chat should be terminated.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">    &quot;&quot;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token keyword" style="font-style:italic">pass</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">groupchat </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">GroupChat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    speaker_selection_method</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">custom_speaker_selection_func</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>The last speaker and the groupchat object are passed to the function.
Commonly used variables from groupchat are <code>groupchat.messages</code> and
<code>groupchat.agents</code>, which is the message history and the agents in the
group chat respectively. You can access other attributes of the
groupchat, such as <code>groupchat.allowed_speaker_transitions_dict</code> for
pre-defined <code>allowed_speaker_transitions_dict</code>.</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> os</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">import</span><span class="token plain"> autogen</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># Put your api key in the environment variable OPENAI_API_KEY</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">config_list </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;model&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;gpt-4-0125-preview&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;api_key&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> os</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">environ</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;OPENAI_API_KEY&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># You can also create an file called &quot;OAI_CONFIG_LIST&quot; and store your config there</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># config_list = autogen.config_list_from_json(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic">#     &quot;OAI_CONFIG_LIST&quot;,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic">#     filter_dict={</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic">#         &quot;model&quot;: [&quot;gpt-4-0125-preview&quot;],</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic">#     },</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># )</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">gpt4_config </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> </span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;cache_seed&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">42</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">  </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># change the cache_seed for different trials</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;temperature&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">0</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;config_list&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> config_list</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;timeout&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">120</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">initializer </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">UserProxyAgent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    name</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;Init&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">coder </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">AssistantAgent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    name</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;Retrieve_Action_1&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    llm_config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">gpt4_config</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    system_message</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&quot;&quot;&quot;You are the Coder. Given a topic, write code to retrieve related papers from the arXiv API, print their title, authors, abstract, and link.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">You write python/shell code to solve tasks. Wrap the code in a code block that specifies the script type. The user can&#x27;t modify your code. So do not suggest incomplete code which requires others to modify. Don&#x27;t use a code block if it&#x27;s not intended to be executed by the executor.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">Don&#x27;t include multiple code blocks in one response. Do not ask others to copy and paste the result. Check the execution result returned by the executor.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">If the result indicates there is an error, fix the error and output the code again. Suggest the full code instead of partial code or code changes. If the error can&#x27;t be fixed or if the task is not solved even after the code is executed successfully, analyze the problem, revisit your assumption, collect additional info you need, and think of a different approach to try.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&quot;&quot;&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">executor </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">UserProxyAgent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    name</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;Retrieve_Action_2&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    system_message</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;Executor. Execute the code written by the Coder and report the result.&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    human_input_mode</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;NEVER&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    code_execution_config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token punctuation" style="color:rgb(199, 146, 234)">{</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;last_n_messages&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token number" style="color:rgb(247, 140, 108)">3</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;work_dir&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;paper&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;use_docker&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"> </span><span class="token boolean" style="color:rgb(255, 88, 116)">False</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token punctuation" style="color:rgb(199, 146, 234)">}</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain">  </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">scientist </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">AssistantAgent</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    name</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;Research_Action_1&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    llm_config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">gpt4_config</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    system_message</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token triple-quoted-string string" style="color:rgb(195, 232, 141)">&quot;&quot;&quot;You are the Scientist. Please categorize papers after seeing their abstracts printed and create a markdown table with Domain, Title, Authors, Summary and Link&quot;&quot;&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token keyword" style="font-style:italic">def</span><span class="token plain"> </span><span class="token function" style="color:rgb(130, 170, 255)">state_transition</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">last_speaker</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> groupchat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    messages </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> groupchat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">messages</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token keyword" style="font-style:italic">if</span><span class="token plain"> last_speaker </span><span class="token keyword" style="font-style:italic">is</span><span class="token plain"> initializer</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># init -&gt; retrieve</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token keyword" style="font-style:italic">return</span><span class="token plain"> coder</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token keyword" style="font-style:italic">elif</span><span class="token plain"> last_speaker </span><span class="token keyword" style="font-style:italic">is</span><span class="token plain"> coder</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># retrieve: action 1 -&gt; action 2</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token keyword" style="font-style:italic">return</span><span class="token plain"> executor</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token keyword" style="font-style:italic">elif</span><span class="token plain"> last_speaker </span><span class="token keyword" style="font-style:italic">is</span><span class="token plain"> executor</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token keyword" style="font-style:italic">if</span><span class="token plain"> messages</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token operator" style="color:rgb(137, 221, 255)">-</span><span class="token number" style="color:rgb(247, 140, 108)">1</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;content&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token plain"> </span><span class="token operator" style="color:rgb(137, 221, 255)">==</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;exitcode: 1&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># retrieve --(execution failed)--&gt; retrieve</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token keyword" style="font-style:italic">return</span><span class="token plain"> coder</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token keyword" style="font-style:italic">else</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># retrieve --(execution success)--&gt; research</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">            </span><span class="token keyword" style="font-style:italic">return</span><span class="token plain"> scientist</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    </span><span class="token keyword" style="font-style:italic">elif</span><span class="token plain"> last_speaker </span><span class="token operator" style="color:rgb(137, 221, 255)">==</span><span class="token plain"> </span><span class="token string" style="color:rgb(195, 232, 141)">&quot;Scientist&quot;</span><span class="token punctuation" style="color:rgb(199, 146, 234)">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token comment" style="color:rgb(105, 112, 152);font-style:italic"># research -&gt; end</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">        </span><span class="token keyword" style="font-style:italic">return</span><span class="token plain"> </span><span class="token boolean" style="color:rgb(255, 88, 116)">None</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">groupchat </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">GroupChat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    agents</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token plain">initializer</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> coder</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> executor</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> scientist</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    messages</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token punctuation" style="color:rgb(199, 146, 234)">[</span><span class="token punctuation" style="color:rgb(199, 146, 234)">]</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    max_round</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token number" style="color:rgb(247, 140, 108)">20</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    speaker_selection_method</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">state_transition</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">manager </span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain"> autogen</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">GroupChatManager</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain">groupchat</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">groupchat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> llm_config</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token plain">gpt4_config</span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">initializer</span><span class="token punctuation" style="color:rgb(199, 146, 234)">.</span><span class="token plain">initiate_chat</span><span class="token punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    manager</span><span class="token punctuation" style="color:rgb(199, 146, 234)">,</span><span class="token plain"> message</span><span class="token operator" style="color:rgb(137, 221, 255)">=</span><span class="token string" style="color:rgb(195, 232, 141)">&quot;Topic: LLM applications papers from last week. Requirement: 5 - 10 papers from different domains.&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">)</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token message-id class-name" style="color:rgb(255, 203, 107)">Init</span><span class="token message-id"> </span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token message-id keyword" style="font-style:italic">to</span><span class="token message-id"> </span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">chat_manager</span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token message-id">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Topic: LLM applications papers from last week. Requirement: 5 - 10 papers from different domains.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">--------------------------------------------------------------------------------</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">Retrieve_Action_1</span><span class="token message-id"> </span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token message-id keyword" style="font-style:italic">to</span><span class="token message-id"> </span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">chat_manager</span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token message-id">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">To retrieve related papers from the arXiv API, we can use Python with the `requests` library to send a query to the API and parse the response. Below is a Python script that searches for papers related to &quot;LLM applications&quot; (Large Language Models applications) from the last week, across different domains, and prints out the required information for 5 to 10 papers.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```python</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">import requests</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">from datetime import datetime, timedelta</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">import feedparser</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># Define the base URL for the arXiv API</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">ARXIV_API_URL = &#x27;http://export.arxiv.org/api/query?&#x27;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># Define the search parameters</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">search_query = &#x27;all:&quot;LLM applications&quot;&#x27;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">start_date = (datetime.now() - timedelta(days=7)).strftime(&#x27;%Y%m%d%H%M%S&#x27;)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">end_date = datetime.now().strftime(&#x27;%Y%m%d%H%M%S&#x27;)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">start = 0</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">max_results = 10</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">sort_by = &#x27;submittedDate&#x27;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">sort_order = &#x27;descending&#x27;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># Construct the query</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">query = f&#x27;search_query={search_query}&amp;sortBy={sort_by}&amp;sortOrder={sort_order}&amp;start={start}&amp;max_results={max_results}&#x27;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># Send the request to the arXiv API</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">response = requests.get(ARXIV_API_URL + query)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># Parse the response using feedparser</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">feed = feedparser.parse(response.content)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># Print the title, authors, abstract, and link of each paper</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">for entry in feed.entries:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    print(&quot;Title:&quot;, entry.title)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    print(&quot;Authors:&quot;, &#x27;, &#x27;.join(author.name for author in entry.authors))</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    print(&quot;Abstract:&quot;, entry.summary)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    print(&quot;Link:&quot;, entry.link)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    print(&quot;\n&quot;)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"># Check if we have at least 5 papers, if not, adjust the search or notify</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">if len(feed.entries) &lt; 5:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">    print(&quot;Less than 5 papers found. Consider adjusting the search parameters or timeframe.&quot;)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">```</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">This script will print the title, authors, abstract, and link for each paper related to &quot;LLM applications&quot; from the last week, up to a maximum of 10 papers. If fewer than 5 papers are found, it will notify the user to consider adjusting the search parameters or timeframe.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">--------------------------------------------------------------------------------</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token function" style="color:rgb(130, 170, 255)">&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; EXECUTING CODE BLOCK 0 (inferred language is python)...</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token function" style="color:rgb(130, 170, 255)"></span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">Retrieve_Action_2</span><span class="token message-id"> </span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token message-id keyword" style="font-style:italic">to</span><span class="token message-id"> </span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">chat_manager</span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token message-id">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">exitcode: 0 (execution succeeded)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Code output: </span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Title: PRSA: Prompt Reverse Stealing Attacks against Large Language Models</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Authors: Yong Yang, Xuhong Zhang, Yi Jiang, Xi Chen, Haoyu Wang, Shouling Ji, Zonghui Wang</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Abstract: Prompt, recognized as crucial intellectual property, enables large language</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">models (LLMs) to perform specific tasks without the need of fine-tuning,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">underscoring their escalating importance. With the rise of prompt-based</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">services, such as prompt marketplaces and LLM applications, providers often</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">display prompts&#x27; capabilities through input-output examples to attract users.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">However, this paradigm raises a pivotal security concern: does the exposure of</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">input-output pairs pose the risk of potential prompt leakage, infringing on the</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">intellectual property rights of the developers? To our knowledge, this problem</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">still has not been comprehensively explored yet. To remedy this gap, in this</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">paper, we perform the first in depth exploration and propose a novel attack</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">framework for reverse-stealing prompts against commercial LLMs, namely PRSA.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">The main idea of PRSA is that by analyzing the critical features of the</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">input-output pairs, we mimic and gradually infer (steal) the target prompts. In</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">detail, PRSA mainly consists of two key phases: prompt mutation and prompt</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">pruning. In the mutation phase, we propose a prompt attention algorithm based</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">on differential feedback to capture these critical features for effectively</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">inferring the target prompts. In the prompt pruning phase, we identify and mask</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">the words dependent on specific inputs, enabling the prompts to accommodate</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">diverse inputs for generalization. Through extensive evaluation, we verify that</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">PRSA poses a severe threat in real world scenarios. We have reported these</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">findings to prompt service providers and actively collaborate with them to take</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">protective measures for prompt copyright.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Link: http://arxiv.org/abs/2402.19200v1</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Title: Political Compass or Spinning Arrow? Towards More Meaningful Evaluations</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  for Values and Opinions in Large Language Models</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Authors: Paul Röttger, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, Dirk Hovy</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Abstract: Much recent work seeks to evaluate values and opinions in large language</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">models (LLMs) using multiple-choice surveys and questionnaires. Most of this</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">work is motivated by concerns around real-world LLM applications. For example,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">politically-biased LLMs may subtly influence society when they are used by</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">millions of people. Such real-world concerns, however, stand in stark contrast</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">to the artificiality of current evaluations: real users do not typically ask</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">LLMs survey questions. Motivated by this discrepancy, we challenge the</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">prevailing constrained evaluation paradigm for values and opinions in LLMs and</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">explore more realistic unconstrained evaluations. As a case study, we focus on</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">the popular Political Compass Test (PCT). In a systematic review, we find that</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">most prior work using the PCT forces models to comply with the PCT&#x27;s</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">multiple-choice format. We show that models give substantively different</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">answers when not forced; that answers change depending on how models are</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">forced; and that answers lack paraphrase robustness. Then, we demonstrate that</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">models give different answers yet again in a more realistic open-ended answer</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">setting. We distill these findings into recommendations and open challenges in</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">evaluating values and opinions in LLMs.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Link: http://arxiv.org/abs/2402.16786v1</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Title: Large Language Models as Urban Residents: An LLM Agent Framework for</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  Personal Mobility Generation</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Authors: Jiawei Wang, Renhe Jiang, Chuang Yang, Zengqing Wu, Makoto Onizuka, Ryosuke Shibasaki, Chuan Xiao</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Abstract: This paper introduces a novel approach using Large Language Models (LLMs)</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">integrated into an agent framework for flexible and efficient personal mobility</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">generation. LLMs overcome the limitations of previous models by efficiently</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">processing semantic data and offering versatility in modeling various tasks.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Our approach addresses the critical need to align LLMs with real-world urban</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">mobility data, focusing on three research questions: aligning LLMs with rich</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">activity data, developing reliable activity generation strategies, and</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">exploring LLM applications in urban mobility. The key technical contribution is</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">a novel LLM agent framework that accounts for individual activity patterns and</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">motivations, including a self-consistency approach to align LLMs with</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">real-world activity data and a retrieval-augmented strategy for interpretable</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">activity generation. In experimental studies, comprehensive validation is</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">performed using real-world data. This research marks the pioneering work of</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">designing an LLM agent framework for activity generation based on real-world</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">human activity data, offering a promising tool for urban mobility analysis.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Link: http://arxiv.org/abs/2402.14744v1</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Title: An Evaluation of Large Language Models in Bioinformatics Research</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Authors: Hengchuang Yin, Zhonghui Gu, Fanhao Wang, Yiparemu Abuduhaibaier, Yanqiao Zhu, Xinming Tu, Xian-Sheng Hua, Xiao Luo, Yizhou Sun</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Abstract: Large language models (LLMs) such as ChatGPT have gained considerable</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">interest across diverse research communities. Their notable ability for text</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">completion and generation has inaugurated a novel paradigm for</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">language-interfaced problem solving. However, the potential and efficacy of</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">these models in bioinformatics remain incompletely explored. In this work, we</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">study the performance LLMs on a wide spectrum of crucial bioinformatics tasks.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">These tasks include the identification of potential coding regions, extraction</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">of named entities for genes and proteins, detection of antimicrobial and</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">anti-cancer peptides, molecular optimization, and resolution of educational</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">bioinformatics problems. Our findings indicate that, given appropriate prompts,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">LLMs like GPT variants can successfully handle most of these tasks. In</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">addition, we provide a thorough analysis of their limitations in the context of</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">complicated bioinformatics tasks. In conclusion, we believe that this work can</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">provide new perspectives and motivate future research in the field of LLMs</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">applications, AI for Science and bioinformatics.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Link: http://arxiv.org/abs/2402.13714v1</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Title: Privacy-Preserving Instructions for Aligning Large Language Models</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Authors: Da Yu, Peter Kairouz, Sewoong Oh, Zheng Xu</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Abstract: Service providers of large language model (LLM) applications collect user</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">instructions in the wild and use them in further aligning LLMs with users&#x27;</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">intentions. These instructions, which potentially contain sensitive</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">information, are annotated by human workers in the process. This poses a new</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">privacy risk not addressed by the typical private optimization. To this end, we</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">propose using synthetic instructions to replace real instructions in data</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">annotation and model fine-tuning. Formal differential privacy is guaranteed by</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">generating those synthetic instructions using privately fine-tuned generators.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Crucial in achieving the desired utility is our novel filtering algorithm that</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">matches the distribution of the synthetic instructions to that of the real</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">ones. In both supervised fine-tuning and reinforcement learning from human</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">feedback, our extensive experiments demonstrate the high utility of the final</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">set of synthetic instructions by showing comparable results to real</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">instructions. In supervised fine-tuning, models trained with private synthetic</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">instructions outperform leading open-source models such as Vicuna.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Link: http://arxiv.org/abs/2402.13659v1</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Title: Ain&#x27;t Misbehavin&#x27; -- Using LLMs to Generate Expressive Robot Behavior in</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  Conversations with the Tabletop Robot Haru</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Authors: Zining Wang, Paul Reisert, Eric Nichols, Randy Gomez</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Abstract: Social robots aim to establish long-term bonds with humans through engaging</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">conversation. However, traditional conversational approaches, reliant on</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">scripted interactions, often fall short in maintaining engaging conversations.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">This paper addresses this limitation by integrating large language models</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">(LLMs) into social robots to achieve more dynamic and expressive conversations.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">We introduce a fully-automated conversation system that leverages LLMs to</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">generate robot responses with expressive behaviors, congruent with the robot&#x27;s</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">personality. We incorporate robot behavior with two modalities: 1) a</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">text-to-speech (TTS) engine capable of various delivery styles, and 2) a</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">library of physical actions for the robot. We develop a custom,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">state-of-the-art emotion recognition model to dynamically select the robot&#x27;s</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">tone of voice and utilize emojis from LLM output as cues for generating robot</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">actions. A demo of our system is available here. To illuminate design and</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">implementation issues, we conduct a pilot study where volunteers chat with a</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">social robot using our proposed system, and we analyze their feedback,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">conducting a rigorous error analysis of chat transcripts. Feedback was</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">overwhelmingly positive, with participants commenting on the robot&#x27;s empathy,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">helpfulness, naturalness, and entertainment. Most negative feedback was due to</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">automatic speech recognition (ASR) errors which had limited impact on</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">conversations. However, we observed a small class of errors, such as the LLM</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">repeating itself or hallucinating fictitious information and human responses,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">that have the potential to derail conversations, raising important issues for</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">LLM application.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Link: http://arxiv.org/abs/2402.11571v1</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Title: Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  in Ophthalmology and LLM-based evaluation using GPT-4</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Authors: Ting Fang Tan, Kabilan Elangovan, Liyuan Jin, Yao Jie, Li Yong, Joshua Lim, Stanley Poh, Wei Yan Ng, Daniel Lim, Yuhe Ke, Nan Liu, Daniel Shu Wei Ting</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Abstract: Purpose: To assess the alignment of GPT-4-based evaluation to human clinician</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">experts, for the evaluation of responses to ophthalmology-related patient</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">queries generated by fine-tuned LLM chatbots. Methods: 400 ophthalmology</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">questions and paired answers were created by ophthalmologists to represent</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">commonly asked patient questions, divided into fine-tuning (368; 92%), and</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">testing (40; 8%). We find-tuned 5 different LLMs, including LLAMA2-7b,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">LLAMA2-7b-Chat, LLAMA2-13b, and LLAMA2-13b-Chat. For the testing dataset,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">additional 8 glaucoma QnA pairs were included. 200 responses to the testing</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">dataset were generated by 5 fine-tuned LLMs for evaluation. A customized</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">clinical evaluation rubric was used to guide GPT-4 evaluation, grounded on</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">clinical accuracy, relevance, patient safety, and ease of understanding. GPT-4</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">evaluation was then compared against ranking by 5 clinicians for clinical</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">alignment. Results: Among all fine-tuned LLMs, GPT-3.5 scored the highest</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">(87.1%), followed by LLAMA2-13b (80.9%), LLAMA2-13b-chat (75.5%),</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">LLAMA2-7b-Chat (70%) and LLAMA2-7b (68.8%) based on the GPT-4 evaluation. GPT-4</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">evaluation demonstrated significant agreement with human clinician rankings,</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">with Spearman and Kendall Tau correlation coefficients of 0.90 and 0.80</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">respectively; while correlation based on Cohen Kappa was more modest at 0.50.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Notably, qualitative analysis and the glaucoma sub-analysis revealed clinical</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">inaccuracies in the LLM-generated responses, which were appropriately</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">identified by the GPT-4 evaluation. Conclusion: The notable clinical alignment</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">of GPT-4 evaluation highlighted its potential to streamline the clinical</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">evaluation of LLM chatbot responses to healthcare-related queries. By</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">complementing the existing clinician-dependent manual grading, this efficient</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">and automated evaluation could assist the validation of future developments in</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">LLM applications for healthcare.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Link: http://arxiv.org/abs/2402.10083v1</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Title: Unmemorization in Large Language Models via Self-Distillation and</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">  Deliberate Imagination</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Authors: Yijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, Ivan Vulić</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Abstract: While displaying impressive generation capabilities across many tasks, Large</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Language Models (LLMs) still struggle with crucial issues of privacy violation</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">and unwanted exposure of sensitive data. This raises an essential question: how</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">should we prevent such undesired behavior of LLMs while maintaining their</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">strong generation and natural language understanding (NLU) capabilities? In</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">this work, we introduce a novel approach termed deliberate imagination in the</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">context of LLM unlearning. Instead of trying to forget memorized data, we</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">employ a self-distillation framework, guiding LLMs to deliberately imagine</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">alternative scenarios. As demonstrated in a wide range of experiments, the</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">proposed method not only effectively unlearns targeted text but also preserves</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">the LLMs&#x27; capabilities in open-ended generation tasks as well as in NLU tasks.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Our results demonstrate the usefulness of this approach across different models</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">and sizes, and also with parameter-efficient fine-tuning, offering a novel</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">pathway to addressing the challenges with private and sensitive data in LLM</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">applications.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Link: http://arxiv.org/abs/2402.10052v1</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Title: Anchor-based Large Language Models</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Authors: Jianhui Pang, Fanghua Ye, Derek F. Wong, Longyue Wang</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Abstract: Large language models (LLMs) predominantly employ decoder-only transformer</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">architectures, necessitating the retention of keys/values information for</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">historical tokens to provide contextual information and avoid redundant</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">computation. However, the substantial size and parameter volume of these LLMs</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">require massive GPU memory. This memory demand increases with the length of the</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">input text, leading to an urgent need for more efficient methods of information</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">storage and processing. This study introduces Anchor-based LLMs (AnLLMs), which</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">utilize an innovative anchor-based self-attention network (AnSAN) and also an</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">anchor-based inference strategy. This approach enables LLMs to compress</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">sequence information into an anchor token, reducing the keys/values cache and</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">enhancing inference efficiency. Experiments on question-answering benchmarks</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">reveal that AnLLMs maintain similar accuracy levels while achieving up to 99%</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">keys/values cache reduction and up to 3.5 times faster inference. Despite a</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">minor compromise in accuracy, the substantial enhancements of AnLLMs employing</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">the AnSAN technique in resource utilization and computational efficiency</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">underscore their potential for practical LLM applications.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Link: http://arxiv.org/abs/2402.07616v2</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Title: T-RAG: Lessons from the LLM Trenches</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Authors: Masoomali Fatehkia, Ji Kim Lucas, Sanjay Chawla</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Abstract: Large Language Models (LLM) have shown remarkable language capabilities</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">fueling attempts to integrate them into applications across a wide range of</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">domains. An important application area is question answering over private</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">enterprise documents where the main considerations are data security, which</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">necessitates applications that can be deployed on-prem, limited computational</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">resources and the need for a robust application that correctly responds to</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">queries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">framework for building LLM-based applications. While building a RAG is</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">relatively straightforward, making it robust and a reliable application</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">requires extensive customization and relatively deep knowledge of the</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">application domain. We share our experiences building and deploying an LLM</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">application for question answering over private organizational documents. Our</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">application combines the use of RAG with a finetuned open-source LLM.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Additionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">to represent entity hierarchies within the organization. This is used to</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">generate a textual description to augment the context when responding to user</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">queries pertaining to entities within the organization&#x27;s hierarchy. Our</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">evaluations show that this combination performs better than a simple RAG or</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">finetuning implementation. Finally, we share some lessons learned based on our</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">experiences building an LLM application for real-world use.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Link: http://arxiv.org/abs/2402.07483v1</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">--------------------------------------------------------------------------------</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">Research_Action_1</span><span class="token message-id"> </span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">(</span><span class="token message-id keyword" style="font-style:italic">to</span><span class="token message-id"> </span><span class="token message-id class-name" style="color:rgb(255, 203, 107)">chat_manager</span><span class="token message-id punctuation" style="color:rgb(199, 146, 234)">)</span><span class="token message-id">:</span><span class="token plain"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">Based on the retrieved abstracts, here is a markdown table categorizing the papers by domain, along with their titles, authors, summaries, and links:</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Domain | Title | Authors | Summary | Link |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">|</span><span class="token punctuation" style="color:rgb(199, 146, 234)">--------</span><span class="token plain">|</span><span class="token punctuation" style="color:rgb(199, 146, 234)">-------</span><span class="token plain">|</span><span class="token punctuation" style="color:rgb(199, 146, 234)">---------</span><span class="token plain">|</span><span class="token punctuation" style="color:rgb(199, 146, 234)">---------</span><span class="token plain">|</span><span class="token punctuation" style="color:rgb(199, 146, 234)">------</span><span class="token plain">|</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Security | PRSA: Prompt Reverse Stealing Attacks against Large Language Models | Yong Yang, Xuhong Zhang, Yi Jiang, Xi Chen, Haoyu Wang, Shouling Ji, Zonghui Wang | The paper explores the security risks associated with exposing input-output pairs of prompts used in LLMs and proposes a novel attack framework, PRSA, to reverse-steal prompts, posing a threat to intellectual property rights. | [Link](http://arxiv.org/abs/2402.19200v1) |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Ethics &amp; Evaluation | Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models | Paul Röttger, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, Dirk Hovy | This work challenges the constrained evaluation paradigm for values and opinions in LLMs and explores more realistic unconstrained evaluations, focusing on the Political Compass Test (PCT). | [Link](http://arxiv.org/abs/2402.16786v1) |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Urban Mobility | Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation | Jiawei Wang, Renhe Jiang, Chuang Yang, Zengqing Wu, Makoto Onizuka, Ryosuke Shibasaki, Chuan Xiao | Introduces an LLM agent framework for personal mobility generation, aligning LLMs with real-world urban mobility data, and offering a tool for urban mobility analysis. | [Link](http://arxiv.org/abs/2402.14744v1) |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Bioinformatics | An Evaluation of Large Language Models in Bioinformatics Research | Hengchuang Yin, Zhonghui Gu, Fanhao Wang, Yiparemu Abuduhaibaier, Yanqiao Zhu, Xinming Tu, Xian-Sheng Hua, Xiao Luo, Yizhou Sun | Evaluates the performance of LLMs on bioinformatics tasks, highlighting their potential and limitations, and motivating future research in LLM applications in bioinformatics. | [Link](http://arxiv.org/abs/2402.13714v1) |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Privacy | Privacy-Preserving Instructions for Aligning Large Language Models | Da Yu, Peter Kairouz, Sewoong Oh, Zheng Xu | Proposes using synthetic instructions generated by privately fine-tuned generators to replace real instructions in data annotation and model fine-tuning, ensuring privacy while maintaining utility. | [Link](http://arxiv.org/abs/2402.13659v1) |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Social Robotics | Ain&#x27;t Misbehavin&#x27; -- Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru | Zining Wang, Paul Reisert, Eric Nichols, Randy Gomez | Integrates LLMs into social robots to generate dynamic and expressive conversations, using a text-to-speech engine and a library of physical actions for the robot. | [Link](http://arxiv.org/abs/2402.11571v1) |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Ophthalmology | Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4 | Ting Fang Tan, Kabilan Elangovan, Liyuan Jin, Yao Jie, Li Yong, Joshua Lim, Stanley Poh, Wei Yan Ng, Daniel Lim, Yuhe Ke, Nan Liu, Daniel Shu Wei Ting | Assesses the alignment of GPT-4-based evaluation to human clinician experts for evaluating responses to ophthalmology-related patient queries generated by fine-tuned LLM chatbots. | [Link](http://arxiv.org/abs/2402.10083v1) |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Privacy &amp; Data Security | Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination | Yijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, Ivan Vulić | Introduces a novel approach for LLM unlearning by guiding LLMs to imagine alternative scenarios, effectively unlearning targeted text while preserving generation and NLU capabilities. | [Link](http://arxiv.org/abs/2402.10052v1) |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Computational Efficiency | Anchor-based Large Language Models | Jianhui Pang, Fanghua Ye, Derek F. Wong, Longyue Wang | Proposes Anchor-based LLMs (AnLLMs) with an innovative anchor-based self-attention network (AnSAN) to reduce memory demand and enhance inference efficiency. | [Link](http://arxiv.org/abs/2402.07616v2) |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">| Enterprise Applications | T-RAG: Lessons from the LLM Trenches | Masoomali Fatehkia, Ji Kim Lucas, Sanjay Chawla | Shares experiences building and deploying an LLM application for question answering over private organizational documents, combining RAG with a finetuned LLM and a tree structure for entity hierarchies. | [Link](http://arxiv.org/abs/2402.07483v1) |</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain">These papers cover a range of domains including security, ethics, urban mobility, bioinformatics, privacy, social robotics, ophthalmology, data security, computational efficiency, and enterprise applications, showcasing the diverse applications of large language models.</span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#bfc7d5"><span class="token plain"></span><span class="token punctuation" style="color:rgb(199, 146, 234)">--------------------------------------------------------------------------------</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#bfc7d5;--prism-background-color:#292d3e"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#bfc7d5;background-color:#292d3e"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#bfc7d5"><span class="token plain">ChatResult(chat_id=None, chat_history=[{&#x27;content&#x27;: &#x27;Topic: LLM applications papers from last week. Requirement: 5 - 10 papers from different domains.&#x27;, &#x27;role&#x27;: &#x27;assistant&#x27;}, {&#x27;content&#x27;: &#x27;To retrieve related papers from the arXiv API, we can use Python with the `requests` library to send a query to the API and parse the response. Below is a Python script that searches for papers related to &quot;LLM applications&quot; (Large Language Models applications) from the last week, across different domains, and prints out the required information for 5 to 10 papers.\n\n```python\nimport requests\nfrom datetime import datetime, timedelta\nimport feedparser\n\n# Define the base URL for the arXiv API\nARXIV_API_URL = \&#x27;http://export.arxiv.org/api/query?\&#x27;\n\n# Define the search parameters\nsearch_query = \&#x27;all:&quot;LLM applications&quot;\&#x27;\nstart_date = (datetime.now() - timedelta(days=7)).strftime(\&#x27;%Y%m%d%H%M%S\&#x27;)\nend_date = datetime.now().strftime(\&#x27;%Y%m%d%H%M%S\&#x27;)\nstart = 0\nmax_results = 10\nsort_by = \&#x27;submittedDate\&#x27;\nsort_order = \&#x27;descending\&#x27;\n\n# Construct the query\nquery = f\&#x27;search_query={search_query}&amp;sortBy={sort_by}&amp;sortOrder={sort_order}&amp;start={start}&amp;max_results={max_results}\&#x27;\n\n# Send the request to the arXiv API\nresponse = requests.get(ARXIV_API_URL + query)\n\n# Parse the response using feedparser\nfeed = feedparser.parse(response.content)\n\n# Print the title, authors, abstract, and link of each paper\nfor entry in feed.entries:\n    print(&quot;Title:&quot;, entry.title)\n    print(&quot;Authors:&quot;, \&#x27;, \&#x27;.join(author.name for author in entry.authors))\n    print(&quot;Abstract:&quot;, entry.summary)\n    print(&quot;Link:&quot;, entry.link)\n    print(&quot;\\n&quot;)\n\n# Check if we have at least 5 papers, if not, adjust the search or notify\nif len(feed.entries) &lt; 5:\n    print(&quot;Less than 5 papers found. Consider adjusting the search parameters or timeframe.&quot;)\n```\n\nThis script will print the title, authors, abstract, and link for each paper related to &quot;LLM applications&quot; from the last week, up to a maximum of 10 papers. If fewer than 5 papers are found, it will notify the user to consider adjusting the search parameters or timeframe.&#x27;, &#x27;name&#x27;: &#x27;Retrieve_Action_1&#x27;, &#x27;role&#x27;: &#x27;user&#x27;}, {&#x27;content&#x27;: &quot;exitcode: 0 (execution succeeded)\nCode output: \nTitle: PRSA: Prompt Reverse Stealing Attacks against Large Language Models\nAuthors: Yong Yang, Xuhong Zhang, Yi Jiang, Xi Chen, Haoyu Wang, Shouling Ji, Zonghui Wang\nAbstract: Prompt, recognized as crucial intellectual property, enables large language\nmodels (LLMs) to perform specific tasks without the need of fine-tuning,\nunderscoring their escalating importance. With the rise of prompt-based\nservices, such as prompt marketplaces and LLM applications, providers often\ndisplay prompts&#x27; capabilities through input-output examples to attract users.\nHowever, this paradigm raises a pivotal security concern: does the exposure of\ninput-output pairs pose the risk of potential prompt leakage, infringing on the\nintellectual property rights of the developers? To our knowledge, this problem\nstill has not been comprehensively explored yet. To remedy this gap, in this\npaper, we perform the first in depth exploration and propose a novel attack\nframework for reverse-stealing prompts against commercial LLMs, namely PRSA.\nThe main idea of PRSA is that by analyzing the critical features of the\ninput-output pairs, we mimic and gradually infer (steal) the target prompts. In\ndetail, PRSA mainly consists of two key phases: prompt mutation and prompt\npruning. In the mutation phase, we propose a prompt attention algorithm based\non differential feedback to capture these critical features for effectively\ninferring the target prompts. In the prompt pruning phase, we identify and mask\nthe words dependent on specific inputs, enabling the prompts to accommodate\ndiverse inputs for generalization. Through extensive evaluation, we verify that\nPRSA poses a severe threat in real world scenarios. We have reported these\nfindings to prompt service providers and actively collaborate with them to take\nprotective measures for prompt copyright.\nLink: http://arxiv.org/abs/2402.19200v1\n\n\nTitle: Political Compass or Spinning Arrow? Towards More Meaningful Evaluations\n  for Values and Opinions in Large Language Models\nAuthors: Paul Röttger, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, Dirk Hovy\nAbstract: Much recent work seeks to evaluate values and opinions in large language\nmodels (LLMs) using multiple-choice surveys and questionnaires. Most of this\nwork is motivated by concerns around real-world LLM applications. For example,\npolitically-biased LLMs may subtly influence society when they are used by\nmillions of people. Such real-world concerns, however, stand in stark contrast\nto the artificiality of current evaluations: real users do not typically ask\nLLMs survey questions. Motivated by this discrepancy, we challenge the\nprevailing constrained evaluation paradigm for values and opinions in LLMs and\nexplore more realistic unconstrained evaluations. As a case study, we focus on\nthe popular Political Compass Test (PCT). In a systematic review, we find that\nmost prior work using the PCT forces models to comply with the PCT&#x27;s\nmultiple-choice format. We show that models give substantively different\nanswers when not forced; that answers change depending on how models are\nforced; and that answers lack paraphrase robustness. Then, we demonstrate that\nmodels give different answers yet again in a more realistic open-ended answer\nsetting. We distill these findings into recommendations and open challenges in\nevaluating values and opinions in LLMs.\nLink: http://arxiv.org/abs/2402.16786v1\n\n\nTitle: Large Language Models as Urban Residents: An LLM Agent Framework for\n  Personal Mobility Generation\nAuthors: Jiawei Wang, Renhe Jiang, Chuang Yang, Zengqing Wu, Makoto Onizuka, Ryosuke Shibasaki, Chuan Xiao\nAbstract: This paper introduces a novel approach using Large Language Models (LLMs)\nintegrated into an agent framework for flexible and efficient personal mobility\ngeneration. LLMs overcome the limitations of previous models by efficiently\nprocessing semantic data and offering versatility in modeling various tasks.\nOur approach addresses the critical need to align LLMs with real-world urban\nmobility data, focusing on three research questions: aligning LLMs with rich\nactivity data, developing reliable activity generation strategies, and\nexploring LLM applications in urban mobility. The key technical contribution is\na novel LLM agent framework that accounts for individual activity patterns and\nmotivations, including a self-consistency approach to align LLMs with\nreal-world activity data and a retrieval-augmented strategy for interpretable\nactivity generation. In experimental studies, comprehensive validation is\nperformed using real-world data. This research marks the pioneering work of\ndesigning an LLM agent framework for activity generation based on real-world\nhuman activity data, offering a promising tool for urban mobility analysis.\nLink: http://arxiv.org/abs/2402.14744v1\n\n\nTitle: An Evaluation of Large Language Models in Bioinformatics Research\nAuthors: Hengchuang Yin, Zhonghui Gu, Fanhao Wang, Yiparemu Abuduhaibaier, Yanqiao Zhu, Xinming Tu, Xian-Sheng Hua, Xiao Luo, Yizhou Sun\nAbstract: Large language models (LLMs) such as ChatGPT have gained considerable\ninterest across diverse research communities. Their notable ability for text\ncompletion and generation has inaugurated a novel paradigm for\nlanguage-interfaced problem solving. However, the potential and efficacy of\nthese models in bioinformatics remain incompletely explored. In this work, we\nstudy the performance LLMs on a wide spectrum of crucial bioinformatics tasks.\nThese tasks include the identification of potential coding regions, extraction\nof named entities for genes and proteins, detection of antimicrobial and\nanti-cancer peptides, molecular optimization, and resolution of educational\nbioinformatics problems. Our findings indicate that, given appropriate prompts,\nLLMs like GPT variants can successfully handle most of these tasks. In\naddition, we provide a thorough analysis of their limitations in the context of\ncomplicated bioinformatics tasks. In conclusion, we believe that this work can\nprovide new perspectives and motivate future research in the field of LLMs\napplications, AI for Science and bioinformatics.\nLink: http://arxiv.org/abs/2402.13714v1\n\n\nTitle: Privacy-Preserving Instructions for Aligning Large Language Models\nAuthors: Da Yu, Peter Kairouz, Sewoong Oh, Zheng Xu\nAbstract: Service providers of large language model (LLM) applications collect user\ninstructions in the wild and use them in further aligning LLMs with users&#x27;\nintentions. These instructions, which potentially contain sensitive\ninformation, are annotated by human workers in the process. This poses a new\nprivacy risk not addressed by the typical private optimization. To this end, we\npropose using synthetic instructions to replace real instructions in data\nannotation and model fine-tuning. Formal differential privacy is guaranteed by\ngenerating those synthetic instructions using privately fine-tuned generators.\nCrucial in achieving the desired utility is our novel filtering algorithm that\nmatches the distribution of the synthetic instructions to that of the real\nones. In both supervised fine-tuning and reinforcement learning from human\nfeedback, our extensive experiments demonstrate the high utility of the final\nset of synthetic instructions by showing comparable results to real\ninstructions. In supervised fine-tuning, models trained with private synthetic\ninstructions outperform leading open-source models such as Vicuna.\nLink: http://arxiv.org/abs/2402.13659v1\n\n\nTitle: Ain&#x27;t Misbehavin&#x27; -- Using LLMs to Generate Expressive Robot Behavior in\n  Conversations with the Tabletop Robot Haru\nAuthors: Zining Wang, Paul Reisert, Eric Nichols, Randy Gomez\nAbstract: Social robots aim to establish long-term bonds with humans through engaging\nconversation. However, traditional conversational approaches, reliant on\nscripted interactions, often fall short in maintaining engaging conversations.\nThis paper addresses this limitation by integrating large language models\n(LLMs) into social robots to achieve more dynamic and expressive conversations.\nWe introduce a fully-automated conversation system that leverages LLMs to\ngenerate robot responses with expressive behaviors, congruent with the robot&#x27;s\npersonality. We incorporate robot behavior with two modalities: 1) a\ntext-to-speech (TTS) engine capable of various delivery styles, and 2) a\nlibrary of physical actions for the robot. We develop a custom,\nstate-of-the-art emotion recognition model to dynamically select the robot&#x27;s\ntone of voice and utilize emojis from LLM output as cues for generating robot\nactions. A demo of our system is available here. To illuminate design and\nimplementation issues, we conduct a pilot study where volunteers chat with a\nsocial robot using our proposed system, and we analyze their feedback,\nconducting a rigorous error analysis of chat transcripts. Feedback was\noverwhelmingly positive, with participants commenting on the robot&#x27;s empathy,\nhelpfulness, naturalness, and entertainment. Most negative feedback was due to\nautomatic speech recognition (ASR) errors which had limited impact on\nconversations. However, we observed a small class of errors, such as the LLM\nrepeating itself or hallucinating fictitious information and human responses,\nthat have the potential to derail conversations, raising important issues for\nLLM application.\nLink: http://arxiv.org/abs/2402.11571v1\n\n\nTitle: Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots\n  in Ophthalmology and LLM-based evaluation using GPT-4\nAuthors: Ting Fang Tan, Kabilan Elangovan, Liyuan Jin, Yao Jie, Li Yong, Joshua Lim, Stanley Poh, Wei Yan Ng, Daniel Lim, Yuhe Ke, Nan Liu, Daniel Shu Wei Ting\nAbstract: Purpose: To assess the alignment of GPT-4-based evaluation to human clinician\nexperts, for the evaluation of responses to ophthalmology-related patient\nqueries generated by fine-tuned LLM chatbots. Methods: 400 ophthalmology\nquestions and paired answers were created by ophthalmologists to represent\ncommonly asked patient questions, divided into fine-tuning (368; 92%), and\ntesting (40; 8%). We find-tuned 5 different LLMs, including LLAMA2-7b,\nLLAMA2-7b-Chat, LLAMA2-13b, and LLAMA2-13b-Chat. For the testing dataset,\nadditional 8 glaucoma QnA pairs were included. 200 responses to the testing\ndataset were generated by 5 fine-tuned LLMs for evaluation. A customized\nclinical evaluation rubric was used to guide GPT-4 evaluation, grounded on\nclinical accuracy, relevance, patient safety, and ease of understanding. GPT-4\nevaluation was then compared against ranking by 5 clinicians for clinical\nalignment. Results: Among all fine-tuned LLMs, GPT-3.5 scored the highest\n(87.1%), followed by LLAMA2-13b (80.9%), LLAMA2-13b-chat (75.5%),\nLLAMA2-7b-Chat (70%) and LLAMA2-7b (68.8%) based on the GPT-4 evaluation. GPT-4\nevaluation demonstrated significant agreement with human clinician rankings,\nwith Spearman and Kendall Tau correlation coefficients of 0.90 and 0.80\nrespectively; while correlation based on Cohen Kappa was more modest at 0.50.\nNotably, qualitative analysis and the glaucoma sub-analysis revealed clinical\ninaccuracies in the LLM-generated responses, which were appropriately\nidentified by the GPT-4 evaluation. Conclusion: The notable clinical alignment\nof GPT-4 evaluation highlighted its potential to streamline the clinical\nevaluation of LLM chatbot responses to healthcare-related queries. By\ncomplementing the existing clinician-dependent manual grading, this efficient\nand automated evaluation could assist the validation of future developments in\nLLM applications for healthcare.\nLink: http://arxiv.org/abs/2402.10083v1\n\n\nTitle: Unmemorization in Large Language Models via Self-Distillation and\n  Deliberate Imagination\nAuthors: Yijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, Ivan Vulić\nAbstract: While displaying impressive generation capabilities across many tasks, Large\nLanguage Models (LLMs) still struggle with crucial issues of privacy violation\nand unwanted exposure of sensitive data. This raises an essential question: how\nshould we prevent such undesired behavior of LLMs while maintaining their\nstrong generation and natural language understanding (NLU) capabilities? In\nthis work, we introduce a novel approach termed deliberate imagination in the\ncontext of LLM unlearning. Instead of trying to forget memorized data, we\nemploy a self-distillation framework, guiding LLMs to deliberately imagine\nalternative scenarios. As demonstrated in a wide range of experiments, the\nproposed method not only effectively unlearns targeted text but also preserves\nthe LLMs&#x27; capabilities in open-ended generation tasks as well as in NLU tasks.\nOur results demonstrate the usefulness of this approach across different models\nand sizes, and also with parameter-efficient fine-tuning, offering a novel\npathway to addressing the challenges with private and sensitive data in LLM\napplications.\nLink: http://arxiv.org/abs/2402.10052v1\n\n\nTitle: Anchor-based Large Language Models\nAuthors: Jianhui Pang, Fanghua Ye, Derek F. Wong, Longyue Wang\nAbstract: Large language models (LLMs) predominantly employ decoder-only transformer\narchitectures, necessitating the retention of keys/values information for\nhistorical tokens to provide contextual information and avoid redundant\ncomputation. However, the substantial size and parameter volume of these LLMs\nrequire massive GPU memory. This memory demand increases with the length of the\ninput text, leading to an urgent need for more efficient methods of information\nstorage and processing. This study introduces Anchor-based LLMs (AnLLMs), which\nutilize an innovative anchor-based self-attention network (AnSAN) and also an\nanchor-based inference strategy. This approach enables LLMs to compress\nsequence information into an anchor token, reducing the keys/values cache and\nenhancing inference efficiency. Experiments on question-answering benchmarks\nreveal that AnLLMs maintain similar accuracy levels while achieving up to 99%\nkeys/values cache reduction and up to 3.5 times faster inference. Despite a\nminor compromise in accuracy, the substantial enhancements of AnLLMs employing\nthe AnSAN technique in resource utilization and computational efficiency\nunderscore their potential for practical LLM applications.\nLink: http://arxiv.org/abs/2402.07616v2\n\n\nTitle: T-RAG: Lessons from the LLM Trenches\nAuthors: Masoomali Fatehkia, Ji Kim Lucas, Sanjay Chawla\nAbstract: Large Language Models (LLM) have shown remarkable language capabilities\nfueling attempts to integrate them into applications across a wide range of\ndomains. An important application area is question answering over private\nenterprise documents where the main considerations are data security, which\nnecessitates applications that can be deployed on-prem, limited computational\nresources and the need for a robust application that correctly responds to\nqueries. Retrieval-Augmented Generation (RAG) has emerged as the most prominent\nframework for building LLM-based applications. While building a RAG is\nrelatively straightforward, making it robust and a reliable application\nrequires extensive customization and relatively deep knowledge of the\napplication domain. We share our experiences building and deploying an LLM\napplication for question answering over private organizational documents. Our\napplication combines the use of RAG with a finetuned open-source LLM.\nAdditionally, our system, which we call Tree-RAG (T-RAG), uses a tree structure\nto represent entity hierarchies within the organization. This is used to\ngenerate a textual description to augment the context when responding to user\nqueries pertaining to entities within the organization&#x27;s hierarchy. Our\nevaluations show that this combination performs better than a simple RAG or\nfinetuning implementation. Finally, we share some lessons learned based on our\nexperiences building an LLM application for real-world use.\nLink: http://arxiv.org/abs/2402.07483v1\n\n\n&quot;, &#x27;name&#x27;: &#x27;Retrieve_Action_2&#x27;, &#x27;role&#x27;: &#x27;user&#x27;}, {&#x27;content&#x27;: &quot;Based on the retrieved abstracts, here is a markdown table categorizing the papers by domain, along with their titles, authors, summaries, and links:\n\n| Domain | Title | Authors | Summary | Link |\n|</span><span class="token punctuation" style="color:rgb(199, 146, 234)">--------</span><span class="token plain">|</span><span class="token punctuation" style="color:rgb(199, 146, 234)">-------</span><span class="token plain">|</span><span class="token punctuation" style="color:rgb(199, 146, 234)">---------</span><span class="token plain">|</span><span class="token punctuation" style="color:rgb(199, 146, 234)">---------</span><span class="token plain">|</span><span class="token punctuation" style="color:rgb(199, 146, 234)">------</span><span class="token plain">|\n| Security | PRSA: Prompt Reverse Stealing Attacks against Large Language Models | Yong Yang, Xuhong Zhang, Yi Jiang, Xi Chen, Haoyu Wang, Shouling Ji, Zonghui Wang | The paper explores the security risks associated with exposing input-output pairs of prompts used in LLMs and proposes a novel attack framework, PRSA, to reverse-steal prompts, posing a threat to intellectual property rights. | [Link](http://arxiv.org/abs/2402.19200v1) |\n| Ethics &amp; Evaluation | Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models | Paul Röttger, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, Dirk Hovy | This work challenges the constrained evaluation paradigm for values and opinions in LLMs and explores more realistic unconstrained evaluations, focusing on the Political Compass Test (PCT). | [Link](http://arxiv.org/abs/2402.16786v1) |\n| Urban Mobility | Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation | Jiawei Wang, Renhe Jiang, Chuang Yang, Zengqing Wu, Makoto Onizuka, Ryosuke Shibasaki, Chuan Xiao | Introduces an LLM agent framework for personal mobility generation, aligning LLMs with real-world urban mobility data, and offering a tool for urban mobility analysis. | [Link](http://arxiv.org/abs/2402.14744v1) |\n| Bioinformatics | An Evaluation of Large Language Models in Bioinformatics Research | Hengchuang Yin, Zhonghui Gu, Fanhao Wang, Yiparemu Abuduhaibaier, Yanqiao Zhu, Xinming Tu, Xian-Sheng Hua, Xiao Luo, Yizhou Sun | Evaluates the performance of LLMs on bioinformatics tasks, highlighting their potential and limitations, and motivating future research in LLM applications in bioinformatics. | [Link](http://arxiv.org/abs/2402.13714v1) |\n| Privacy | Privacy-Preserving Instructions for Aligning Large Language Models | Da Yu, Peter Kairouz, Sewoong Oh, Zheng Xu | Proposes using synthetic instructions generated by privately fine-tuned generators to replace real instructions in data annotation and model fine-tuning, ensuring privacy while maintaining utility. | [Link](http://arxiv.org/abs/2402.13659v1) |\n| Social Robotics | Ain&#x27;t Misbehavin&#x27; -- Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru | Zining Wang, Paul Reisert, Eric Nichols, Randy Gomez | Integrates LLMs into social robots to generate dynamic and expressive conversations, using a text-to-speech engine and a library of physical actions for the robot. | [Link](http://arxiv.org/abs/2402.11571v1) |\n| Ophthalmology | Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4 | Ting Fang Tan, Kabilan Elangovan, Liyuan Jin, Yao Jie, Li Yong, Joshua Lim, Stanley Poh, Wei Yan Ng, Daniel Lim, Yuhe Ke, Nan Liu, Daniel Shu Wei Ting | Assesses the alignment of GPT-4-based evaluation to human clinician experts for evaluating responses to ophthalmology-related patient queries generated by fine-tuned LLM chatbots. | [Link](http://arxiv.org/abs/2402.10083v1) |\n| Privacy &amp; Data Security | Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination | Yijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, Ivan Vulić | Introduces a novel approach for LLM unlearning by guiding LLMs to imagine alternative scenarios, effectively unlearning targeted text while preserving generation and NLU capabilities. | [Link](http://arxiv.org/abs/2402.10052v1) |\n| Computational Efficiency | Anchor-based Large Language Models | Jianhui Pang, Fanghua Ye, Derek F. Wong, Longyue Wang | Proposes Anchor-based LLMs (AnLLMs) with an innovative anchor-based self-attention network (AnSAN) to reduce memory demand and enhance inference efficiency. | [Link](http://arxiv.org/abs/2402.07616v2) |\n| Enterprise Applications | T-RAG: Lessons from the LLM Trenches | Masoomali Fatehkia, Ji Kim Lucas, Sanjay Chawla | Shares experiences building and deploying an LLM application for question answering over private organizational documents, combining RAG with a finetuned LLM and a tree structure for entity hierarchies. | [Link](http://arxiv.org/abs/2402.07483v1) |\n\nThese papers cover a range of domains including security, ethics, urban mobility, bioinformatics, privacy, social robotics, ophthalmology, data security, computational efficiency, and enterprise applications, showcasing the diverse applications of large language models.&quot;, &#x27;name&#x27;: &#x27;Research_Action_1&#x27;, &#x27;role&#x27;: &#x27;user&#x27;}], summary=&quot;Based on the retrieved abstracts, here is a markdown table categorizing the papers by domain, along with their titles, authors, summaries, and links:\n\n| Domain | Title | Authors | Summary | Link |\n|</span><span class="token punctuation" style="color:rgb(199, 146, 234)">--------</span><span class="token plain">|</span><span class="token punctuation" style="color:rgb(199, 146, 234)">-------</span><span class="token plain">|</span><span class="token punctuation" style="color:rgb(199, 146, 234)">---------</span><span class="token plain">|</span><span class="token punctuation" style="color:rgb(199, 146, 234)">---------</span><span class="token plain">|</span><span class="token punctuation" style="color:rgb(199, 146, 234)">------</span><span class="token plain">|\n| Security | PRSA: Prompt Reverse Stealing Attacks against Large Language Models | Yong Yang, Xuhong Zhang, Yi Jiang, Xi Chen, Haoyu Wang, Shouling Ji, Zonghui Wang | The paper explores the security risks associated with exposing input-output pairs of prompts used in LLMs and proposes a novel attack framework, PRSA, to reverse-steal prompts, posing a threat to intellectual property rights. | [Link](http://arxiv.org/abs/2402.19200v1) |\n| Ethics &amp; Evaluation | Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models | Paul Röttger, Valentin Hofmann, Valentina Pyatkin, Musashi Hinck, Hannah Rose Kirk, Hinrich Schütze, Dirk Hovy | This work challenges the constrained evaluation paradigm for values and opinions in LLMs and explores more realistic unconstrained evaluations, focusing on the Political Compass Test (PCT). | [Link](http://arxiv.org/abs/2402.16786v1) |\n| Urban Mobility | Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation | Jiawei Wang, Renhe Jiang, Chuang Yang, Zengqing Wu, Makoto Onizuka, Ryosuke Shibasaki, Chuan Xiao | Introduces an LLM agent framework for personal mobility generation, aligning LLMs with real-world urban mobility data, and offering a tool for urban mobility analysis. | [Link](http://arxiv.org/abs/2402.14744v1) |\n| Bioinformatics | An Evaluation of Large Language Models in Bioinformatics Research | Hengchuang Yin, Zhonghui Gu, Fanhao Wang, Yiparemu Abuduhaibaier, Yanqiao Zhu, Xinming Tu, Xian-Sheng Hua, Xiao Luo, Yizhou Sun | Evaluates the performance of LLMs on bioinformatics tasks, highlighting their potential and limitations, and motivating future research in LLM applications in bioinformatics. | [Link](http://arxiv.org/abs/2402.13714v1) |\n| Privacy | Privacy-Preserving Instructions for Aligning Large Language Models | Da Yu, Peter Kairouz, Sewoong Oh, Zheng Xu | Proposes using synthetic instructions generated by privately fine-tuned generators to replace real instructions in data annotation and model fine-tuning, ensuring privacy while maintaining utility. | [Link](http://arxiv.org/abs/2402.13659v1) |\n| Social Robotics | Ain&#x27;t Misbehavin&#x27; -- Using LLMs to Generate Expressive Robot Behavior in Conversations with the Tabletop Robot Haru | Zining Wang, Paul Reisert, Eric Nichols, Randy Gomez | Integrates LLMs into social robots to generate dynamic and expressive conversations, using a text-to-speech engine and a library of physical actions for the robot. | [Link](http://arxiv.org/abs/2402.11571v1) |\n| Ophthalmology | Fine-tuning Large Language Model (LLM) Artificial Intelligence Chatbots in Ophthalmology and LLM-based evaluation using GPT-4 | Ting Fang Tan, Kabilan Elangovan, Liyuan Jin, Yao Jie, Li Yong, Joshua Lim, Stanley Poh, Wei Yan Ng, Daniel Lim, Yuhe Ke, Nan Liu, Daniel Shu Wei Ting | Assesses the alignment of GPT-4-based evaluation to human clinician experts for evaluating responses to ophthalmology-related patient queries generated by fine-tuned LLM chatbots. | [Link](http://arxiv.org/abs/2402.10083v1) |\n| Privacy &amp; Data Security | Unmemorization in Large Language Models via Self-Distillation and Deliberate Imagination | Yijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, Ivan Vulić | Introduces a novel approach for LLM unlearning by guiding LLMs to imagine alternative scenarios, effectively unlearning targeted text while preserving generation and NLU capabilities. | [Link](http://arxiv.org/abs/2402.10052v1) |\n| Computational Efficiency | Anchor-based Large Language Models | Jianhui Pang, Fanghua Ye, Derek F. Wong, Longyue Wang | Proposes Anchor-based LLMs (AnLLMs) with an innovative anchor-based self-attention network (AnSAN) to reduce memory demand and enhance inference efficiency. | [Link](http://arxiv.org/abs/2402.07616v2) |\n| Enterprise Applications | T-RAG: Lessons from the LLM Trenches | Masoomali Fatehkia, Ji Kim Lucas, Sanjay Chawla | Shares experiences building and deploying an LLM application for question answering over private organizational documents, combining RAG with a finetuned LLM and a tree structure for entity hierarchies. | [Link](http://arxiv.org/abs/2402.07483v1) |\n\nThese papers cover a range of domains including security, ethics, urban mobility, bioinformatics, privacy, social robotics, ophthalmology, data security, computational efficiency, and enterprise applications, showcasing the diverse applications of large language models.&quot;, cost=({&#x27;total_cost&#x27;: 0}, {&#x27;total_cost&#x27;: 0}), human_input=[])</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-tags-row row margin-bottom--sm"><div class="col"><b>Tags:</b><ul class="tags_jXut padding--none margin-left--sm"><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/autogen/docs/tags/orchestration">orchestration</a></li><li class="tag_QGVx"><a class="tag_zVej tagRegular_sFm0" href="/autogen/docs/tags/group-chat">group chat</a></li></ul></div></div><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/microsoft/autogen/edit/main/website/docs/topics/groupchat/customized_speaker_selection.ipynb" target="_blank" rel="noopener noreferrer" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_Z9Sw" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_vwxv"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/autogen/docs/topics/openai-assistant/gpt_assistant_agent"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Agent Backed by OpenAI Assistant API</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/autogen/docs/topics/groupchat/resuming_groupchat"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Resuming a GroupChat</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#an-example-research-workflow" class="table-of-contents__link toc-highlight">An example research workflow</a></li><li><a href="#create-your-speaker-selection-function" class="table-of-contents__link toc-highlight">Create your speaker selection function</a></li></ul></div></div></div></div></main></div></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://aka.ms/autogen-dc" target="_blank" rel="noopener noreferrer" class="footer__link-item">Discord<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://twitter.com/pyautogen" target="_blank" rel="noopener noreferrer" class="footer__link-item">Twitter<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright © 2024 AutoGen Authors |  <a target="_blank" style="color:#10adff" href="https://go.microsoft.com/fwlink/?LinkId=521839">Privacy and Cookies</a></div></div></div></footer></div>
</body>
</html>